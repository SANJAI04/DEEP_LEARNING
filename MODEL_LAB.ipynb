{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed97d679",
   "metadata": {
    "id": "ed97d679"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import SimpleRNN,Input,Activation,Reshape,LSTM,GRU,Conv2D,MaxPooling2D,Flatten,Embedding,Dense,Dropout,UpSampling2D\n",
    "import torch\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1200d",
   "metadata": {
    "id": "b6e1200d"
   },
   "source": [
    "####  Logic Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15136255",
   "metadata": {
    "id": "15136255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAUT(1, 1) = 1\n",
      "TAUT(1, 0) = 1\n",
      "TAUT(0, 1) = 1\n",
      "TAUT(0, 0) = 1\n",
      "NOT(0) = 1\n",
      "NOT(1) = 0\n",
      "AND(1, 1) = 1\n",
      "AND(1, 0) = 0\n",
      "AND(0, 1) = 0\n",
      "AND(0, 0) = 0\n",
      "OR(1, 1) = 1\n",
      "OR(1, 0) = 1\n",
      "OR(0, 1) = 1\n",
      "OR(0, 0) = 0\n",
      "XOR(1, 1) = 0\n",
      "XOR(1, 0) = 1\n",
      "XOR(0, 1) = 1\n",
      "XOR(0, 0) = 0\n",
      "NOR(1, 1) = 0\n",
      "NOR(1, 0) = 0\n",
      "NOR(0, 1) = 0\n",
      "NOR(0, 0) = 1\n",
      "NOR(1, 1) = 0\n",
      "NOR(1, 0) = 1\n",
      "NOR(0, 1) = 1\n",
      "NOR(0, 0) = 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_step(s):\n",
    "    if s >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def perceptron(x, w, b):\n",
    "    s = np.dot(w, x) + b\n",
    "    y = unit_step(s)\n",
    "    return y\n",
    "\n",
    "def TAUT_percep(x1,x2):  \n",
    "    w = np.array([0, 0])\n",
    "    b = 1\n",
    "    x = np.array([x1,x2])\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "print(\"TAUT({}, {}) = {}\".format(1, 1, TAUT_percep(1,1)))\n",
    "print(\"TAUT({}, {}) = {}\".format(1, 0, TAUT_percep(1,0)))\n",
    "print(\"TAUT({}, {}) = {}\".format(0, 1, TAUT_percep(0,1)))\n",
    "print(\"TAUT({}, {}) = {}\".format(0, 0, TAUT_percep(0,0)))\n",
    "\n",
    "def NOT_percep(x):    \n",
    "    return perceptron(x, w=-1, b=0.5)\n",
    "\n",
    "print(\"NOT(0) = {}\".format(NOT_percep(0)))\n",
    "print(\"NOT(1) = {}\".format(NOT_percep(1)))\n",
    "\n",
    "def AND_percep(x1,x2):\n",
    "    w = np.array([1, 1])\n",
    "    b =-1.5\n",
    "    x = np.array([x1,x2])\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_percep(1,1)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_percep(1,0)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_percep(0,1)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_percep(0,0)))\n",
    "\n",
    "def OR_percep(x1,x2):\n",
    "    w = np.array([1, 1])\n",
    "    b =-0.5\n",
    "    x = np.array([x1,x2])\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_percep(1,1)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_percep(1,0)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_percep(0,1)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_percep(0,0)))\n",
    "\n",
    "def XOR_net(x1,x2):\n",
    "    out_1 = AND_percep(x1,x2)\n",
    "    out_2 = NOT_percep(out_1)\n",
    "    out_3 = OR_percep(x1,x2)\n",
    "    output = AND_percep(out_2,out_3)\n",
    "    return output\n",
    "\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_net(1,1)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_net(1,0)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_net(0,1)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_net(0,0)))\n",
    "\n",
    "def NOR_net(x1,x2):\n",
    "    out_1 = OR_percep(x1,x2)\n",
    "    output = NOT_percep(out_1)\n",
    "    return output\n",
    "\n",
    "print(\"NOR({}, {}) = {}\".format(1, 1, NOR_net(1,1)))\n",
    "print(\"NOR({}, {}) = {}\".format(1, 0, NOR_net(1,0)))\n",
    "print(\"NOR({}, {}) = {}\".format(0, 1, NOR_net(0,1)))\n",
    "print(\"NOR({}, {}) = {}\".format(0, 0, NOR_net(0,0)))\n",
    "\n",
    "def NAND_net(x1,x2):\n",
    "    out_1 = AND_percep(x1,x2)\n",
    "    output = NOT_percep(out_1)\n",
    "    return output\n",
    "\n",
    "print(\"NOR({}, {}) = {}\".format(1, 1, NAND_net(1,1)))\n",
    "print(\"NOR({}, {}) = {}\".format(1, 0, NAND_net(1,0)))\n",
    "print(\"NOR({}, {}) = {}\".format(0, 1, NAND_net(0,1)))\n",
    "print(\"NOR({}, {}) = {}\".format(0, 0, NAND_net(0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59513c",
   "metadata": {
    "id": "ff59513c"
   },
   "source": [
    "### XNOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b823a1f",
   "metadata": {
    "id": "1b823a1f"
   },
   "source": [
    "1 Input Layer <br>\n",
    "1 Hidden Layer - 2 Neurons <br>\n",
    "1 Output Layer - 1 Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c45080",
   "metadata": {
    "id": "d3c45080",
    "outputId": "78db77d0-0db8-40d5-f08f-461cd4bf0b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[1],[0],[0],[1]])\n",
    "\n",
    "w1 = np.array([[1,-1],[1,-1]])\n",
    "w2 = np.array([[1],[1]])\n",
    "\n",
    "bias = np.array([[-1,1]])\n",
    "\n",
    "y_cap = step(step(x@w1 + bias)@w2)\n",
    "y_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35886b",
   "metadata": {
    "id": "ae35886b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080b48e",
   "metadata": {
    "id": "6080b48e"
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfad32",
   "metadata": {
    "id": "b9dfad32"
   },
   "outputs": [],
   "source": [
    "mean = x_train.mean()\n",
    "x_train = x_train - mean\n",
    "std = x_train.std()\n",
    "x_train/=std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9907870",
   "metadata": {
    "id": "a9907870"
   },
   "outputs": [],
   "source": [
    "x_test -= mean\n",
    "x_test /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e377881",
   "metadata": {
    "id": "8e377881",
    "outputId": "2bd54e4c-dac8-4e5e-e731-b7488f0d6fc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2a684",
   "metadata": {
    "id": "6ea2a684"
   },
   "source": [
    "### MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431a0ef",
   "metadata": {
    "collapsed": true,
    "id": "0431a0ef",
    "outputId": "408e6815-3546-48df-c724-55487c48a42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "13/13 [==============================] - 1s 20ms/step - loss: 139.0666 - val_loss: 76.8312\n",
      "Epoch 2/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 62.1396 - val_loss: 50.8851\n",
      "Epoch 3/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 40.5181 - val_loss: 40.7767\n",
      "Epoch 4/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 33.1572 - val_loss: 37.7680\n",
      "Epoch 5/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.4480 - val_loss: 36.5413\n",
      "Epoch 6/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.0195 - val_loss: 35.9174\n",
      "Epoch 7/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8785 - val_loss: 35.6408\n",
      "Epoch 8/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8472 - val_loss: 35.6277\n",
      "Epoch 9/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8731 - val_loss: 35.5813\n",
      "Epoch 10/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8820 - val_loss: 35.6672\n",
      "Epoch 11/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8699 - val_loss: 35.7118\n",
      "Epoch 12/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9116 - val_loss: 35.6478\n",
      "Epoch 13/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9081 - val_loss: 35.7112\n",
      "Epoch 14/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8952 - val_loss: 35.5889\n",
      "Epoch 15/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8489 - val_loss: 35.4892\n",
      "Epoch 16/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8661 - val_loss: 35.4336\n",
      "Epoch 17/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8825 - val_loss: 35.4351\n",
      "Epoch 18/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8908 - val_loss: 35.5996\n",
      "Epoch 19/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8610 - val_loss: 35.5091\n",
      "Epoch 20/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8782 - val_loss: 35.5005\n",
      "Epoch 21/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8476 - val_loss: 35.5071\n",
      "Epoch 22/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8837 - val_loss: 35.4777\n",
      "Epoch 23/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8661 - val_loss: 35.5638\n",
      "Epoch 24/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8387 - val_loss: 35.5960\n",
      "Epoch 25/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8854 - val_loss: 35.7152\n",
      "Epoch 26/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8643 - val_loss: 35.7275\n",
      "Epoch 27/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8705 - val_loss: 35.6263\n",
      "Epoch 28/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8675 - val_loss: 35.6274\n",
      "Epoch 29/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8534 - val_loss: 35.5837\n",
      "Epoch 30/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8803 - val_loss: 35.5165\n",
      "Epoch 31/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8441 - val_loss: 35.6324\n",
      "Epoch 32/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8554 - val_loss: 35.6472\n",
      "Epoch 33/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8378 - val_loss: 35.5920\n",
      "Epoch 34/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8739 - val_loss: 35.6647\n",
      "Epoch 35/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8538 - val_loss: 35.6262\n",
      "Epoch 36/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8669 - val_loss: 35.6260\n",
      "Epoch 37/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8577 - val_loss: 35.6121\n",
      "Epoch 38/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8513 - val_loss: 35.5615\n",
      "Epoch 39/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8693 - val_loss: 35.5163\n",
      "Epoch 40/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8690 - val_loss: 35.5153\n",
      "Epoch 41/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8822 - val_loss: 35.5320\n",
      "Epoch 42/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8613 - val_loss: 35.5438\n",
      "Epoch 43/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8653 - val_loss: 35.6011\n",
      "Epoch 44/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8615 - val_loss: 35.6319\n",
      "Epoch 45/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8627 - val_loss: 35.7610\n",
      "Epoch 46/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8608 - val_loss: 35.6141\n",
      "Epoch 47/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8849 - val_loss: 35.6089\n",
      "Epoch 48/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8927 - val_loss: 35.6325\n",
      "Epoch 49/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8810 - val_loss: 35.6651\n",
      "Epoch 50/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8484 - val_loss: 35.6257\n",
      "Epoch 51/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8442 - val_loss: 35.6425\n",
      "Epoch 52/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8523 - val_loss: 35.7531\n",
      "Epoch 53/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8529 - val_loss: 35.6918\n",
      "Epoch 54/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8521 - val_loss: 35.6158\n",
      "Epoch 55/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8760 - val_loss: 35.6980\n",
      "Epoch 56/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8619 - val_loss: 35.6099\n",
      "Epoch 57/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8682 - val_loss: 35.6425\n",
      "Epoch 58/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8729 - val_loss: 35.5625\n",
      "Epoch 59/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8827 - val_loss: 35.5780\n",
      "Epoch 60/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8642 - val_loss: 35.6559\n",
      "Epoch 61/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8570 - val_loss: 35.6714\n",
      "Epoch 62/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8626 - val_loss: 35.5480\n",
      "Epoch 63/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8775 - val_loss: 35.6615\n",
      "Epoch 64/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.9207 - val_loss: 35.6016\n",
      "Epoch 65/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8777 - val_loss: 35.6584\n",
      "Epoch 66/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8868 - val_loss: 35.6119\n",
      "Epoch 67/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8936 - val_loss: 35.5838\n",
      "Epoch 68/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8602 - val_loss: 35.6330\n",
      "Epoch 69/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8737 - val_loss: 35.7287\n",
      "Epoch 70/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8580 - val_loss: 35.6800\n",
      "Epoch 71/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8820 - val_loss: 35.7265\n",
      "Epoch 72/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8653 - val_loss: 35.6305\n",
      "Epoch 73/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8841 - val_loss: 35.6548\n",
      "Epoch 74/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8608 - val_loss: 35.6017\n",
      "Epoch 75/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8543 - val_loss: 35.4472\n",
      "Epoch 76/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9005 - val_loss: 35.6430\n",
      "Epoch 77/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8859 - val_loss: 35.5646\n",
      "Epoch 78/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8896 - val_loss: 35.5474\n",
      "Epoch 79/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8487 - val_loss: 35.4861\n",
      "Epoch 80/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8677 - val_loss: 35.5951\n",
      "Epoch 81/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8546 - val_loss: 35.6381\n",
      "Epoch 82/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8655 - val_loss: 35.6115\n",
      "Epoch 83/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8673 - val_loss: 35.6445\n",
      "Epoch 84/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8919 - val_loss: 35.5857\n",
      "Epoch 85/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8572 - val_loss: 35.5356\n",
      "Epoch 86/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8683 - val_loss: 35.5591\n",
      "Epoch 87/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8642 - val_loss: 35.5858\n",
      "Epoch 88/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8651 - val_loss: 35.6464\n",
      "Epoch 89/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8545 - val_loss: 35.5557\n",
      "Epoch 90/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8619 - val_loss: 35.6483\n",
      "Epoch 91/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 30.9025 - val_loss: 35.5347\n",
      "Epoch 92/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8644 - val_loss: 35.5976\n",
      "Epoch 93/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8471 - val_loss: 35.5375\n",
      "Epoch 94/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8802 - val_loss: 35.5398\n",
      "Epoch 95/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8579 - val_loss: 35.6028\n",
      "Epoch 96/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8616 - val_loss: 35.6568\n",
      "Epoch 97/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8563 - val_loss: 35.6198\n",
      "Epoch 98/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8670 - val_loss: 35.5444\n",
      "Epoch 99/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8688 - val_loss: 35.5321\n",
      "Epoch 100/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8573 - val_loss: 35.5745\n",
      "Epoch 101/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8673 - val_loss: 35.5768\n",
      "Epoch 102/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8767 - val_loss: 35.6443\n",
      "Epoch 103/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8440 - val_loss: 35.5781\n",
      "Epoch 104/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8737 - val_loss: 35.5430\n",
      "Epoch 105/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8821 - val_loss: 35.4967\n",
      "Epoch 106/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8347 - val_loss: 35.6329\n",
      "Epoch 107/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8962 - val_loss: 35.5772\n",
      "Epoch 108/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8926 - val_loss: 35.5379\n",
      "Epoch 109/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8664 - val_loss: 35.6010\n",
      "Epoch 110/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8922 - val_loss: 35.5955\n",
      "Epoch 111/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8632 - val_loss: 35.5870\n",
      "Epoch 112/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8999 - val_loss: 35.5460\n",
      "Epoch 113/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8567 - val_loss: 35.5219\n",
      "Epoch 114/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9438 - val_loss: 35.5529\n",
      "Epoch 115/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8517 - val_loss: 35.6080\n",
      "Epoch 116/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8696 - val_loss: 35.6720\n",
      "Epoch 117/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8820 - val_loss: 35.5204\n",
      "Epoch 118/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8706 - val_loss: 35.5162\n",
      "Epoch 119/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8763 - val_loss: 35.4207\n",
      "Epoch 120/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9183 - val_loss: 35.5040\n",
      "Epoch 121/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8718 - val_loss: 35.5853\n",
      "Epoch 122/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8680 - val_loss: 35.5778\n",
      "Epoch 123/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8730 - val_loss: 35.5446\n",
      "Epoch 124/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8624 - val_loss: 35.6051\n",
      "Epoch 125/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8533 - val_loss: 35.4954\n",
      "Epoch 126/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8553 - val_loss: 35.5220\n",
      "Epoch 127/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8679 - val_loss: 35.5271\n",
      "Epoch 128/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8315 - val_loss: 35.6239\n",
      "Epoch 129/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8459 - val_loss: 35.5510\n",
      "Epoch 130/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8659 - val_loss: 35.5978\n",
      "Epoch 131/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8641 - val_loss: 35.5511\n",
      "Epoch 132/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8549 - val_loss: 35.5719\n",
      "Epoch 133/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8582 - val_loss: 35.5865\n",
      "Epoch 134/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8564 - val_loss: 35.5639\n",
      "Epoch 135/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8604 - val_loss: 35.4936\n",
      "Epoch 136/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8511 - val_loss: 35.5618\n",
      "Epoch 137/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9322 - val_loss: 35.4531\n",
      "Epoch 138/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8835 - val_loss: 35.4563\n",
      "Epoch 139/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8738 - val_loss: 35.3632\n",
      "Epoch 140/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9180 - val_loss: 35.4546\n",
      "Epoch 141/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8768 - val_loss: 35.5422\n",
      "Epoch 142/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8895 - val_loss: 35.5350\n",
      "Epoch 143/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8453 - val_loss: 35.7464\n",
      "Epoch 144/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8685 - val_loss: 35.6317\n",
      "Epoch 145/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8997 - val_loss: 35.5836\n",
      "Epoch 146/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9158 - val_loss: 35.4723\n",
      "Epoch 147/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8535 - val_loss: 35.4299\n",
      "Epoch 148/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8619 - val_loss: 35.5310\n",
      "Epoch 149/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8733 - val_loss: 35.6224\n",
      "Epoch 150/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8628 - val_loss: 35.6135\n",
      "Epoch 151/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8702 - val_loss: 35.5820\n",
      "Epoch 152/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8747 - val_loss: 35.6246\n",
      "Epoch 153/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8273 - val_loss: 35.7257\n",
      "Epoch 154/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8580 - val_loss: 35.6524\n",
      "Epoch 155/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8434 - val_loss: 35.6205\n",
      "Epoch 156/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8731 - val_loss: 35.6031\n",
      "Epoch 157/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8960 - val_loss: 35.5561\n",
      "Epoch 158/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8749 - val_loss: 35.5392\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8687 - val_loss: 35.6392\n",
      "Epoch 160/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8999 - val_loss: 35.6828\n",
      "Epoch 161/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9036 - val_loss: 35.7167\n",
      "Epoch 162/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8701 - val_loss: 35.6338\n",
      "Epoch 163/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8444 - val_loss: 35.6062\n",
      "Epoch 164/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8521 - val_loss: 35.6851\n",
      "Epoch 165/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8936 - val_loss: 35.5382\n",
      "Epoch 166/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8527 - val_loss: 35.4675\n",
      "Epoch 167/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8959 - val_loss: 35.5292\n",
      "Epoch 168/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8916 - val_loss: 35.5975\n",
      "Epoch 169/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8841 - val_loss: 35.6040\n",
      "Epoch 170/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8739 - val_loss: 35.7793\n",
      "Epoch 171/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8844 - val_loss: 35.7700\n",
      "Epoch 172/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8985 - val_loss: 35.7029\n",
      "Epoch 173/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8615 - val_loss: 35.5406\n",
      "Epoch 174/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8698 - val_loss: 35.5621\n",
      "Epoch 175/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8349 - val_loss: 35.5903\n",
      "Epoch 176/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8747 - val_loss: 35.5997\n",
      "Epoch 177/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8493 - val_loss: 35.6518\n",
      "Epoch 178/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8935 - val_loss: 35.7111\n",
      "Epoch 179/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8523 - val_loss: 35.7660\n",
      "Epoch 180/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8577 - val_loss: 35.6823\n",
      "Epoch 181/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8691 - val_loss: 35.6360\n",
      "Epoch 182/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8822 - val_loss: 35.6207\n",
      "Epoch 183/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8856 - val_loss: 35.5841\n",
      "Epoch 184/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8792 - val_loss: 35.5820\n",
      "Epoch 185/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8438 - val_loss: 35.5067\n",
      "Epoch 186/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8682 - val_loss: 35.5032\n",
      "Epoch 187/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8600 - val_loss: 35.6660\n",
      "Epoch 188/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8600 - val_loss: 35.6156\n",
      "Epoch 189/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8612 - val_loss: 35.6940\n",
      "Epoch 190/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8525 - val_loss: 35.5941\n",
      "Epoch 191/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8926 - val_loss: 35.5550\n",
      "Epoch 192/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8420 - val_loss: 35.5543\n",
      "Epoch 193/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8883 - val_loss: 35.5074\n",
      "Epoch 194/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8699 - val_loss: 35.4610\n",
      "Epoch 195/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8483 - val_loss: 35.4657\n",
      "Epoch 196/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8535 - val_loss: 35.5873\n",
      "Epoch 197/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8590 - val_loss: 35.5149\n",
      "Epoch 198/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8730 - val_loss: 35.6204\n",
      "Epoch 199/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8527 - val_loss: 35.5959\n",
      "Epoch 200/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8447 - val_loss: 35.5431\n",
      "Epoch 201/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8670 - val_loss: 35.5370\n",
      "Epoch 202/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8543 - val_loss: 35.4892\n",
      "Epoch 203/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8486 - val_loss: 35.5603\n",
      "Epoch 204/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8606 - val_loss: 35.5496\n",
      "Epoch 205/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8942 - val_loss: 35.5427\n",
      "Epoch 206/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8842 - val_loss: 35.5949\n",
      "Epoch 207/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8442 - val_loss: 35.6760\n",
      "Epoch 208/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8743 - val_loss: 35.5697\n",
      "Epoch 209/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8582 - val_loss: 35.4976\n",
      "Epoch 210/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8655 - val_loss: 35.5680\n",
      "Epoch 211/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8570 - val_loss: 35.4239\n",
      "Epoch 212/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8864 - val_loss: 35.3646\n",
      "Epoch 213/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9118 - val_loss: 35.4514\n",
      "Epoch 214/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8477 - val_loss: 35.5144\n",
      "Epoch 215/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8783 - val_loss: 35.5242\n",
      "Epoch 216/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8836 - val_loss: 35.5016\n",
      "Epoch 217/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8550 - val_loss: 35.5254\n",
      "Epoch 218/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8732 - val_loss: 35.5641\n",
      "Epoch 219/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8728 - val_loss: 35.5799\n",
      "Epoch 220/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8547 - val_loss: 35.5365\n",
      "Epoch 221/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8807 - val_loss: 35.5030\n",
      "Epoch 222/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8940 - val_loss: 35.6077\n",
      "Epoch 223/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8824 - val_loss: 35.6436\n",
      "Epoch 224/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8474 - val_loss: 35.4907\n",
      "Epoch 225/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8832 - val_loss: 35.4345\n",
      "Epoch 226/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.9316 - val_loss: 35.5651\n",
      "Epoch 227/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8609 - val_loss: 35.5965\n",
      "Epoch 228/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8817 - val_loss: 35.6198\n",
      "Epoch 229/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8628 - val_loss: 35.5380\n",
      "Epoch 230/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8526 - val_loss: 35.6022\n",
      "Epoch 231/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8518 - val_loss: 35.5699\n",
      "Epoch 232/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9170 - val_loss: 35.5778\n",
      "Epoch 233/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8773 - val_loss: 35.5793\n",
      "Epoch 234/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8604 - val_loss: 35.6470\n",
      "Epoch 235/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8720 - val_loss: 35.7708\n",
      "Epoch 236/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8810 - val_loss: 35.6829\n",
      "Epoch 237/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8643 - val_loss: 35.7076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8878 - val_loss: 35.5018\n",
      "Epoch 239/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9024 - val_loss: 35.5138\n",
      "Epoch 240/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8548 - val_loss: 35.6048\n",
      "Epoch 241/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8667 - val_loss: 35.6281\n",
      "Epoch 242/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8901 - val_loss: 35.5833\n",
      "Epoch 243/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8881 - val_loss: 35.5716\n",
      "Epoch 244/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8928 - val_loss: 35.5634\n",
      "Epoch 245/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8917 - val_loss: 35.5821\n",
      "Epoch 246/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8511 - val_loss: 35.6377\n",
      "Epoch 247/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8935 - val_loss: 35.4854\n",
      "Epoch 248/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8385 - val_loss: 35.5275\n",
      "Epoch 249/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8486 - val_loss: 35.5755\n",
      "Epoch 250/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8700 - val_loss: 35.4126\n",
      "Epoch 251/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8802 - val_loss: 35.4307\n",
      "Epoch 252/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8605 - val_loss: 35.5222\n",
      "Epoch 253/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8672 - val_loss: 35.4650\n",
      "Epoch 254/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8642 - val_loss: 35.6050\n",
      "Epoch 255/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8604 - val_loss: 35.7430\n",
      "Epoch 256/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8587 - val_loss: 35.6938\n",
      "Epoch 257/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8600 - val_loss: 35.6070\n",
      "Epoch 258/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8664 - val_loss: 35.5669\n",
      "Epoch 259/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8485 - val_loss: 35.5068\n",
      "Epoch 260/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8517 - val_loss: 35.4855\n",
      "Epoch 261/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8625 - val_loss: 35.4960\n",
      "Epoch 262/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8676 - val_loss: 35.5192\n",
      "Epoch 263/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8585 - val_loss: 35.6252\n",
      "Epoch 264/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8935 - val_loss: 35.5337\n",
      "Epoch 265/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8673 - val_loss: 35.5025\n",
      "Epoch 266/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8767 - val_loss: 35.5109\n",
      "Epoch 267/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8569 - val_loss: 35.5557\n",
      "Epoch 268/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8506 - val_loss: 35.5563\n",
      "Epoch 269/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8771 - val_loss: 35.5658\n",
      "Epoch 270/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8575 - val_loss: 35.5345\n",
      "Epoch 271/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8436 - val_loss: 35.6892\n",
      "Epoch 272/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8901 - val_loss: 35.6054\n",
      "Epoch 273/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8434 - val_loss: 35.5439\n",
      "Epoch 274/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8741 - val_loss: 35.5740\n",
      "Epoch 275/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8865 - val_loss: 35.5573\n",
      "Epoch 276/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8446 - val_loss: 35.5086\n",
      "Epoch 277/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8503 - val_loss: 35.5625\n",
      "Epoch 278/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8552 - val_loss: 35.5528\n",
      "Epoch 279/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9161 - val_loss: 35.5008\n",
      "Epoch 280/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8629 - val_loss: 35.4568\n",
      "Epoch 281/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8518 - val_loss: 35.5104\n",
      "Epoch 282/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8748 - val_loss: 35.5012\n",
      "Epoch 283/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8511 - val_loss: 35.5956\n",
      "Epoch 284/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8551 - val_loss: 35.5627\n",
      "Epoch 285/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8481 - val_loss: 35.4841\n",
      "Epoch 286/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8456 - val_loss: 35.6609\n",
      "Epoch 287/1000\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 30.8506 - val_loss: 35.5780\n",
      "Epoch 288/1000\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 30.8977 - val_loss: 35.5668\n",
      "Epoch 289/1000\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 30.8774 - val_loss: 35.4988\n",
      "Epoch 290/1000\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 30.8802 - val_loss: 35.5542\n",
      "Epoch 291/1000\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 30.8459 - val_loss: 35.5279\n",
      "Epoch 292/1000\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 30.8608 - val_loss: 35.6210\n",
      "Epoch 293/1000\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 30.9049 - val_loss: 35.6825\n",
      "Epoch 294/1000\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 30.8810 - val_loss: 35.5632\n",
      "Epoch 295/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8645 - val_loss: 35.6438\n",
      "Epoch 296/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8668 - val_loss: 35.6327\n",
      "Epoch 297/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9293 - val_loss: 35.6717\n",
      "Epoch 298/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8656 - val_loss: 35.5936\n",
      "Epoch 299/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8656 - val_loss: 35.5772\n",
      "Epoch 300/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8689 - val_loss: 35.6142\n",
      "Epoch 301/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8577 - val_loss: 35.6136\n",
      "Epoch 302/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8822 - val_loss: 35.5819\n",
      "Epoch 303/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8641 - val_loss: 35.5663\n",
      "Epoch 304/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8703 - val_loss: 35.6191\n",
      "Epoch 305/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8507 - val_loss: 35.5210\n",
      "Epoch 306/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.9084 - val_loss: 35.4982\n",
      "Epoch 307/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9082 - val_loss: 35.5574\n",
      "Epoch 308/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8469 - val_loss: 35.6577\n",
      "Epoch 309/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8743 - val_loss: 35.6263\n",
      "Epoch 310/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8733 - val_loss: 35.6996\n",
      "Epoch 311/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8517 - val_loss: 35.6229\n",
      "Epoch 312/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8591 - val_loss: 35.5845\n",
      "Epoch 313/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8807 - val_loss: 35.5918\n",
      "Epoch 314/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8635 - val_loss: 35.5919\n",
      "Epoch 315/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8770 - val_loss: 35.6602\n",
      "Epoch 316/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8569 - val_loss: 35.7474\n",
      "Epoch 317/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8548 - val_loss: 35.6447\n",
      "Epoch 318/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8718 - val_loss: 35.7750\n",
      "Epoch 319/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8715 - val_loss: 35.6687\n",
      "Epoch 320/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8649 - val_loss: 35.5765\n",
      "Epoch 321/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8754 - val_loss: 35.5453\n",
      "Epoch 322/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8613 - val_loss: 35.5964\n",
      "Epoch 323/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8506 - val_loss: 35.5442\n",
      "Epoch 324/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8747 - val_loss: 35.6006\n",
      "Epoch 325/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9008 - val_loss: 35.5569\n",
      "Epoch 326/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8597 - val_loss: 35.4564\n",
      "Epoch 327/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8489 - val_loss: 35.5022\n",
      "Epoch 328/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8695 - val_loss: 35.4913\n",
      "Epoch 329/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8725 - val_loss: 35.5784\n",
      "Epoch 330/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8611 - val_loss: 35.6090\n",
      "Epoch 331/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8636 - val_loss: 35.5403\n",
      "Epoch 332/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8744 - val_loss: 35.4556\n",
      "Epoch 333/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.9037 - val_loss: 35.5424\n",
      "Epoch 334/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8660 - val_loss: 35.5118\n",
      "Epoch 335/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8665 - val_loss: 35.4586\n",
      "Epoch 336/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8521 - val_loss: 35.4585\n",
      "Epoch 337/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8954 - val_loss: 35.5086\n",
      "Epoch 338/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8838 - val_loss: 35.5664\n",
      "Epoch 339/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8436 - val_loss: 35.5575\n",
      "Epoch 340/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8732 - val_loss: 35.6155\n",
      "Epoch 341/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9119 - val_loss: 35.7658\n",
      "Epoch 342/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8845 - val_loss: 35.6433\n",
      "Epoch 343/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8726 - val_loss: 35.6305\n",
      "Epoch 344/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8457 - val_loss: 35.6032\n",
      "Epoch 345/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8604 - val_loss: 35.5790\n",
      "Epoch 346/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8628 - val_loss: 35.6147\n",
      "Epoch 347/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8452 - val_loss: 35.5130\n",
      "Epoch 348/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8378 - val_loss: 35.6380\n",
      "Epoch 349/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8503 - val_loss: 35.6984\n",
      "Epoch 350/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8644 - val_loss: 35.5898\n",
      "Epoch 351/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8434 - val_loss: 35.5948\n",
      "Epoch 352/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8637 - val_loss: 35.6098\n",
      "Epoch 353/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8661 - val_loss: 35.5154\n",
      "Epoch 354/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8557 - val_loss: 35.5164\n",
      "Epoch 355/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8557 - val_loss: 35.5830\n",
      "Epoch 356/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8482 - val_loss: 35.5714\n",
      "Epoch 357/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8407 - val_loss: 35.6334\n",
      "Epoch 358/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8899 - val_loss: 35.6185\n",
      "Epoch 359/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8901 - val_loss: 35.6532\n",
      "Epoch 360/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8616 - val_loss: 35.6312\n",
      "Epoch 361/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8674 - val_loss: 35.6668\n",
      "Epoch 362/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8603 - val_loss: 35.7723\n",
      "Epoch 363/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8593 - val_loss: 35.7308\n",
      "Epoch 364/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8570 - val_loss: 35.7231\n",
      "Epoch 365/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8462 - val_loss: 35.4942\n",
      "Epoch 366/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8578 - val_loss: 35.5189\n",
      "Epoch 367/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8484 - val_loss: 35.5459\n",
      "Epoch 368/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8723 - val_loss: 35.5320\n",
      "Epoch 369/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8663 - val_loss: 35.5982\n",
      "Epoch 370/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8622 - val_loss: 35.6075\n",
      "Epoch 371/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8491 - val_loss: 35.5998\n",
      "Epoch 372/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9126 - val_loss: 35.7050\n",
      "Epoch 373/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8591 - val_loss: 35.6365\n",
      "Epoch 374/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8722 - val_loss: 35.5783\n",
      "Epoch 375/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8894 - val_loss: 35.4595\n",
      "Epoch 376/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8697 - val_loss: 35.4878\n",
      "Epoch 377/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8735 - val_loss: 35.5064\n",
      "Epoch 378/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8506 - val_loss: 35.5728\n",
      "Epoch 379/1000\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 30.8914 - val_loss: 35.5416\n",
      "Epoch 380/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8606 - val_loss: 35.5618\n",
      "Epoch 381/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8692 - val_loss: 35.3996\n",
      "Epoch 382/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8649 - val_loss: 35.5169\n",
      "Epoch 383/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8610 - val_loss: 35.5143\n",
      "Epoch 384/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8719 - val_loss: 35.5381\n",
      "Epoch 385/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8510 - val_loss: 35.5216\n",
      "Epoch 386/1000\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 30.8404 - val_loss: 35.4952\n",
      "Epoch 387/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 30.9201 - val_loss: 35.4949\n",
      "Epoch 388/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8636 - val_loss: 35.6108\n",
      "Epoch 389/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8616 - val_loss: 35.5961\n",
      "Epoch 390/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8763 - val_loss: 35.6549\n",
      "Epoch 391/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8975 - val_loss: 35.5568\n",
      "Epoch 392/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8581 - val_loss: 35.6883\n",
      "Epoch 393/1000\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 30.8601 - val_loss: 35.5918\n",
      "Epoch 394/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8539 - val_loss: 35.5351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8681 - val_loss: 35.5170\n",
      "Epoch 396/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8503 - val_loss: 35.5639\n",
      "Epoch 397/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8797 - val_loss: 35.5745\n",
      "Epoch 398/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8720 - val_loss: 35.6541\n",
      "Epoch 399/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8774 - val_loss: 35.5716\n",
      "Epoch 400/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8492 - val_loss: 35.6589\n",
      "Epoch 401/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8791 - val_loss: 35.7256\n",
      "Epoch 402/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8610 - val_loss: 35.7026\n",
      "Epoch 403/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8803 - val_loss: 35.7028\n",
      "Epoch 404/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8640 - val_loss: 35.7504\n",
      "Epoch 405/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8550 - val_loss: 35.5838\n",
      "Epoch 406/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8571 - val_loss: 35.6360\n",
      "Epoch 407/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8774 - val_loss: 35.5238\n",
      "Epoch 408/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8989 - val_loss: 35.4718\n",
      "Epoch 409/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9148 - val_loss: 35.5805\n",
      "Epoch 410/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.9162 - val_loss: 35.4862\n",
      "Epoch 411/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8679 - val_loss: 35.5476\n",
      "Epoch 412/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8626 - val_loss: 35.5183\n",
      "Epoch 413/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8986 - val_loss: 35.4953\n",
      "Epoch 414/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8480 - val_loss: 35.5056\n",
      "Epoch 415/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8808 - val_loss: 35.6400\n",
      "Epoch 416/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8825 - val_loss: 35.6430\n",
      "Epoch 417/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8667 - val_loss: 35.6987\n",
      "Epoch 418/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8656 - val_loss: 35.5962\n",
      "Epoch 419/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8977 - val_loss: 35.6368\n",
      "Epoch 420/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8695 - val_loss: 35.6142\n",
      "Epoch 421/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8370 - val_loss: 35.5641\n",
      "Epoch 422/1000\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 30.8529 - val_loss: 35.4542\n",
      "Epoch 423/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8424 - val_loss: 35.5056\n",
      "Epoch 424/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8593 - val_loss: 35.5659\n",
      "Epoch 425/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8522 - val_loss: 35.6382\n",
      "Epoch 426/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8914 - val_loss: 35.5961\n",
      "Epoch 427/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8581 - val_loss: 35.6234\n",
      "Epoch 428/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8833 - val_loss: 35.5646\n",
      "Epoch 429/1000\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 30.8625 - val_loss: 35.4875\n",
      "Epoch 430/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8435 - val_loss: 35.5470\n",
      "Epoch 431/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8569 - val_loss: 35.6091\n",
      "Epoch 432/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8691 - val_loss: 35.5593\n",
      "Epoch 433/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8529 - val_loss: 35.6957\n",
      "Epoch 434/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8680 - val_loss: 35.6255\n",
      "Epoch 435/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8572 - val_loss: 35.5744\n",
      "Epoch 436/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8439 - val_loss: 35.4985\n",
      "Epoch 437/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8792 - val_loss: 35.4585\n",
      "Epoch 438/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8778 - val_loss: 35.4667\n",
      "Epoch 439/1000\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 30.8756 - val_loss: 35.5829\n",
      "Epoch 440/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8712 - val_loss: 35.5494\n",
      "Epoch 441/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8478 - val_loss: 35.4654\n",
      "Epoch 442/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8487 - val_loss: 35.5363\n",
      "Epoch 443/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8569 - val_loss: 35.6156\n",
      "Epoch 444/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8378 - val_loss: 35.5734\n",
      "Epoch 445/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8791 - val_loss: 35.4839\n",
      "Epoch 446/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8932 - val_loss: 35.5442\n",
      "Epoch 447/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8789 - val_loss: 35.6451\n",
      "Epoch 448/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9011 - val_loss: 35.5427\n",
      "Epoch 449/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8793 - val_loss: 35.4974\n",
      "Epoch 450/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8502 - val_loss: 35.5192\n",
      "Epoch 451/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8852 - val_loss: 35.5365\n",
      "Epoch 452/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8785 - val_loss: 35.5939\n",
      "Epoch 453/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9056 - val_loss: 35.5656\n",
      "Epoch 454/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8445 - val_loss: 35.6150\n",
      "Epoch 455/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8551 - val_loss: 35.6252\n",
      "Epoch 456/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8881 - val_loss: 35.6641\n",
      "Epoch 457/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9042 - val_loss: 35.5591\n",
      "Epoch 458/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8494 - val_loss: 35.6489\n",
      "Epoch 459/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8778 - val_loss: 35.7346\n",
      "Epoch 460/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8588 - val_loss: 35.6188\n",
      "Epoch 461/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8645 - val_loss: 35.5544\n",
      "Epoch 462/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8504 - val_loss: 35.4599\n",
      "Epoch 463/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8581 - val_loss: 35.5210\n",
      "Epoch 464/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8493 - val_loss: 35.5551\n",
      "Epoch 465/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8861 - val_loss: 35.5286\n",
      "Epoch 466/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8853 - val_loss: 35.5867\n",
      "Epoch 467/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8848 - val_loss: 35.5152\n",
      "Epoch 468/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8673 - val_loss: 35.5670\n",
      "Epoch 469/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8409 - val_loss: 35.5852\n",
      "Epoch 470/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8355 - val_loss: 35.6086\n",
      "Epoch 471/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8582 - val_loss: 35.6354\n",
      "Epoch 472/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8507 - val_loss: 35.4793\n",
      "Epoch 473/1000\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 30.8360 - val_loss: 35.7383\n",
      "Epoch 474/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.8497 - val_loss: 35.6627\n",
      "Epoch 475/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8467 - val_loss: 35.5535\n",
      "Epoch 476/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8531 - val_loss: 35.5250\n",
      "Epoch 477/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8737 - val_loss: 35.4813\n",
      "Epoch 478/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8923 - val_loss: 35.5943\n",
      "Epoch 479/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.9108 - val_loss: 35.4980\n",
      "Epoch 480/1000\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 30.8609 - val_loss: 35.5079\n",
      "Epoch 481/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8610 - val_loss: 35.5873\n",
      "Epoch 482/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8407 - val_loss: 35.6428\n",
      "Epoch 483/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8909 - val_loss: 35.5642\n",
      "Epoch 484/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8731 - val_loss: 35.5669\n",
      "Epoch 485/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8854 - val_loss: 35.5010\n",
      "Epoch 486/1000\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 30.8621 - val_loss: 35.4996\n",
      "Epoch 487/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8730 - val_loss: 35.4498\n",
      "Epoch 488/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8545 - val_loss: 35.4778\n",
      "Epoch 489/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8401 - val_loss: 35.4612\n",
      "Epoch 490/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8789 - val_loss: 35.4909\n",
      "Epoch 491/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 30.9000 - val_loss: 35.5409\n",
      "Epoch 492/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 30.8640 - val_loss: 35.5872\n",
      "Epoch 493/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9030 - val_loss: 35.6260\n",
      "Epoch 494/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8763 - val_loss: 35.6049\n",
      "Epoch 495/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8603 - val_loss: 35.5052\n",
      "Epoch 496/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8671 - val_loss: 35.4359\n",
      "Epoch 497/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8867 - val_loss: 35.5035\n",
      "Epoch 498/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8899 - val_loss: 35.4880\n",
      "Epoch 499/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8549 - val_loss: 35.5563\n",
      "Epoch 500/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8570 - val_loss: 35.4847\n",
      "Epoch 501/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8620 - val_loss: 35.5164\n",
      "Epoch 502/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8768 - val_loss: 35.6341\n",
      "Epoch 503/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.8684 - val_loss: 35.6154\n",
      "Epoch 504/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 30.8841 - val_loss: 35.5562\n",
      "Epoch 505/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8676 - val_loss: 35.6213\n",
      "Epoch 506/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8529 - val_loss: 35.5738\n",
      "Epoch 507/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8456 - val_loss: 35.5328\n",
      "Epoch 508/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8496 - val_loss: 35.4810\n",
      "Epoch 509/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8886 - val_loss: 35.4792\n",
      "Epoch 510/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8759 - val_loss: 35.5757\n",
      "Epoch 511/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8869 - val_loss: 35.4790\n",
      "Epoch 512/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8763 - val_loss: 35.6131\n",
      "Epoch 513/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8612 - val_loss: 35.7281\n",
      "Epoch 514/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8497 - val_loss: 35.6403\n",
      "Epoch 515/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9033 - val_loss: 35.7804\n",
      "Epoch 516/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8489 - val_loss: 35.5817\n",
      "Epoch 517/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8574 - val_loss: 35.5445\n",
      "Epoch 518/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9276 - val_loss: 35.4544\n",
      "Epoch 519/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8623 - val_loss: 35.4847\n",
      "Epoch 520/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8791 - val_loss: 35.4702\n",
      "Epoch 521/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8739 - val_loss: 35.6060\n",
      "Epoch 522/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.9095 - val_loss: 35.6969\n",
      "Epoch 523/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8621 - val_loss: 35.6157\n",
      "Epoch 524/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8551 - val_loss: 35.5651\n",
      "Epoch 525/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8838 - val_loss: 35.5536\n",
      "Epoch 526/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8793 - val_loss: 35.5514\n",
      "Epoch 527/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8734 - val_loss: 35.5528\n",
      "Epoch 528/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8455 - val_loss: 35.5962\n",
      "Epoch 529/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8594 - val_loss: 35.5219\n",
      "Epoch 530/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8614 - val_loss: 35.4999\n",
      "Epoch 531/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8890 - val_loss: 35.4927\n",
      "Epoch 532/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8775 - val_loss: 35.5251\n",
      "Epoch 533/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8777 - val_loss: 35.4771\n",
      "Epoch 534/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8765 - val_loss: 35.4756\n",
      "Epoch 535/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8533 - val_loss: 35.5858\n",
      "Epoch 536/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8839 - val_loss: 35.5859\n",
      "Epoch 537/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8873 - val_loss: 35.5880\n",
      "Epoch 538/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8662 - val_loss: 35.4701\n",
      "Epoch 539/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8980 - val_loss: 35.5448\n",
      "Epoch 540/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8727 - val_loss: 35.6404\n",
      "Epoch 541/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8888 - val_loss: 35.6707\n",
      "Epoch 542/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8598 - val_loss: 35.5969\n",
      "Epoch 543/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8804 - val_loss: 35.6097\n",
      "Epoch 544/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8988 - val_loss: 35.5126\n",
      "Epoch 545/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8730 - val_loss: 35.5218\n",
      "Epoch 546/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8636 - val_loss: 35.6107\n",
      "Epoch 547/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8724 - val_loss: 35.5900\n",
      "Epoch 548/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8650 - val_loss: 35.6322\n",
      "Epoch 549/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8686 - val_loss: 35.6403\n",
      "Epoch 550/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8562 - val_loss: 35.5375\n",
      "Epoch 551/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8614 - val_loss: 35.5640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 552/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8601 - val_loss: 35.5780\n",
      "Epoch 553/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8430 - val_loss: 35.7768\n",
      "Epoch 554/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9092 - val_loss: 35.6307\n",
      "Epoch 555/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8544 - val_loss: 35.6067\n",
      "Epoch 556/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8611 - val_loss: 35.5512\n",
      "Epoch 557/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8740 - val_loss: 35.5886\n",
      "Epoch 558/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8670 - val_loss: 35.5894\n",
      "Epoch 559/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8985 - val_loss: 35.4746\n",
      "Epoch 560/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8566 - val_loss: 35.5491\n",
      "Epoch 561/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8634 - val_loss: 35.6534\n",
      "Epoch 562/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8820 - val_loss: 35.5781\n",
      "Epoch 563/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8883 - val_loss: 35.6421\n",
      "Epoch 564/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8550 - val_loss: 35.6096\n",
      "Epoch 565/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8850 - val_loss: 35.6284\n",
      "Epoch 566/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8864 - val_loss: 35.5815\n",
      "Epoch 567/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8814 - val_loss: 35.5873\n",
      "Epoch 568/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8387 - val_loss: 35.5474\n",
      "Epoch 569/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9022 - val_loss: 35.6630\n",
      "Epoch 570/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8448 - val_loss: 35.6062\n",
      "Epoch 571/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9241 - val_loss: 35.5443\n",
      "Epoch 572/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8736 - val_loss: 35.7479\n",
      "Epoch 573/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8857 - val_loss: 35.6514\n",
      "Epoch 574/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8456 - val_loss: 35.6315\n",
      "Epoch 575/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8696 - val_loss: 35.5937\n",
      "Epoch 576/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8968 - val_loss: 35.6065\n",
      "Epoch 577/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8673 - val_loss: 35.6173\n",
      "Epoch 578/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8686 - val_loss: 35.6112\n",
      "Epoch 579/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8807 - val_loss: 35.6597\n",
      "Epoch 580/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8702 - val_loss: 35.7135\n",
      "Epoch 581/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8625 - val_loss: 35.6707\n",
      "Epoch 582/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8899 - val_loss: 35.5823\n",
      "Epoch 583/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8575 - val_loss: 35.6113\n",
      "Epoch 584/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8449 - val_loss: 35.6188\n",
      "Epoch 585/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8538 - val_loss: 35.4836\n",
      "Epoch 586/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8466 - val_loss: 35.5914\n",
      "Epoch 587/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8744 - val_loss: 35.5651\n",
      "Epoch 588/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8866 - val_loss: 35.5627\n",
      "Epoch 589/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8581 - val_loss: 35.6156\n",
      "Epoch 590/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8497 - val_loss: 35.7915\n",
      "Epoch 591/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8756 - val_loss: 35.6068\n",
      "Epoch 592/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8580 - val_loss: 35.5815\n",
      "Epoch 593/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8518 - val_loss: 35.6508\n",
      "Epoch 594/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8655 - val_loss: 35.6330\n",
      "Epoch 595/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8797 - val_loss: 35.6598\n",
      "Epoch 596/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8840 - val_loss: 35.7902\n",
      "Epoch 597/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8846 - val_loss: 35.6568\n",
      "Epoch 598/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8861 - val_loss: 35.6325\n",
      "Epoch 599/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8529 - val_loss: 35.5615\n",
      "Epoch 600/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8516 - val_loss: 35.7169\n",
      "Epoch 601/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8867 - val_loss: 35.6163\n",
      "Epoch 602/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8552 - val_loss: 35.7031\n",
      "Epoch 603/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9027 - val_loss: 35.6925\n",
      "Epoch 604/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8706 - val_loss: 35.6481\n",
      "Epoch 605/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8753 - val_loss: 35.7348\n",
      "Epoch 606/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9070 - val_loss: 35.7365\n",
      "Epoch 607/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8551 - val_loss: 35.6010\n",
      "Epoch 608/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8773 - val_loss: 35.6068\n",
      "Epoch 609/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8478 - val_loss: 35.7385\n",
      "Epoch 610/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8710 - val_loss: 35.6461\n",
      "Epoch 611/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8778 - val_loss: 35.7421\n",
      "Epoch 612/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9265 - val_loss: 35.6590\n",
      "Epoch 613/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8569 - val_loss: 35.5546\n",
      "Epoch 614/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8612 - val_loss: 35.5961\n",
      "Epoch 615/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8423 - val_loss: 35.5833\n",
      "Epoch 616/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8449 - val_loss: 35.5122\n",
      "Epoch 617/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8967 - val_loss: 35.6791\n",
      "Epoch 618/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8853 - val_loss: 35.6137\n",
      "Epoch 619/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8751 - val_loss: 35.7245\n",
      "Epoch 620/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8599 - val_loss: 35.6209\n",
      "Epoch 621/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8670 - val_loss: 35.5286\n",
      "Epoch 622/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8523 - val_loss: 35.5657\n",
      "Epoch 623/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9020 - val_loss: 35.5103\n",
      "Epoch 624/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8600 - val_loss: 35.4968\n",
      "Epoch 625/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8692 - val_loss: 35.6409\n",
      "Epoch 626/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8795 - val_loss: 35.5699\n",
      "Epoch 627/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8591 - val_loss: 35.5120\n",
      "Epoch 628/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8570 - val_loss: 35.6519\n",
      "Epoch 629/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8701 - val_loss: 35.4921\n",
      "Epoch 630/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8600 - val_loss: 35.4464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 631/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8941 - val_loss: 35.5044\n",
      "Epoch 632/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8569 - val_loss: 35.5466\n",
      "Epoch 633/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8838 - val_loss: 35.7231\n",
      "Epoch 634/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9026 - val_loss: 35.8089\n",
      "Epoch 635/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8608 - val_loss: 35.7164\n",
      "Epoch 636/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8614 - val_loss: 35.7182\n",
      "Epoch 637/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8687 - val_loss: 35.5937\n",
      "Epoch 638/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8510 - val_loss: 35.7007\n",
      "Epoch 639/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8905 - val_loss: 35.5314\n",
      "Epoch 640/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8714 - val_loss: 35.5412\n",
      "Epoch 641/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9118 - val_loss: 35.6251\n",
      "Epoch 642/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8668 - val_loss: 35.5929\n",
      "Epoch 643/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8500 - val_loss: 35.5763\n",
      "Epoch 644/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8603 - val_loss: 35.6086\n",
      "Epoch 645/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8799 - val_loss: 35.5451\n",
      "Epoch 646/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8863 - val_loss: 35.4737\n",
      "Epoch 647/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8750 - val_loss: 35.4431\n",
      "Epoch 648/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8728 - val_loss: 35.5989\n",
      "Epoch 649/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8668 - val_loss: 35.5223\n",
      "Epoch 650/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8669 - val_loss: 35.5070\n",
      "Epoch 651/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8672 - val_loss: 35.5073\n",
      "Epoch 652/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8451 - val_loss: 35.6539\n",
      "Epoch 653/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9908 - val_loss: 35.6493\n",
      "Epoch 654/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8467 - val_loss: 35.5481\n",
      "Epoch 655/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8762 - val_loss: 35.5227\n",
      "Epoch 656/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8868 - val_loss: 35.5364\n",
      "Epoch 657/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8558 - val_loss: 35.5421\n",
      "Epoch 658/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8673 - val_loss: 35.4657\n",
      "Epoch 659/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8860 - val_loss: 35.5032\n",
      "Epoch 660/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8591 - val_loss: 35.5513\n",
      "Epoch 661/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8886 - val_loss: 35.6034\n",
      "Epoch 662/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8641 - val_loss: 35.5256\n",
      "Epoch 663/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8648 - val_loss: 35.5696\n",
      "Epoch 664/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8565 - val_loss: 35.6103\n",
      "Epoch 665/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8446 - val_loss: 35.5100\n",
      "Epoch 666/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8524 - val_loss: 35.4843\n",
      "Epoch 667/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8657 - val_loss: 35.4650\n",
      "Epoch 668/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8803 - val_loss: 35.5451\n",
      "Epoch 669/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8851 - val_loss: 35.5144\n",
      "Epoch 670/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8440 - val_loss: 35.6071\n",
      "Epoch 671/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8502 - val_loss: 35.6161\n",
      "Epoch 672/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8705 - val_loss: 35.6535\n",
      "Epoch 673/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8817 - val_loss: 35.5247\n",
      "Epoch 674/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8406 - val_loss: 35.5103\n",
      "Epoch 675/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8420 - val_loss: 35.5416\n",
      "Epoch 676/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8520 - val_loss: 35.4986\n",
      "Epoch 677/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8470 - val_loss: 35.4932\n",
      "Epoch 678/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8694 - val_loss: 35.6482\n",
      "Epoch 679/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 30.8675 - val_loss: 35.6217\n",
      "Epoch 680/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8673 - val_loss: 35.5192\n",
      "Epoch 681/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8645 - val_loss: 35.5862\n",
      "Epoch 682/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8807 - val_loss: 35.6092\n",
      "Epoch 683/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8832 - val_loss: 35.6621\n",
      "Epoch 684/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8810 - val_loss: 35.6555\n",
      "Epoch 685/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9043 - val_loss: 35.6523\n",
      "Epoch 686/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8405 - val_loss: 35.6161\n",
      "Epoch 687/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8623 - val_loss: 35.7336\n",
      "Epoch 688/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8565 - val_loss: 35.7200\n",
      "Epoch 689/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8798 - val_loss: 35.6990\n",
      "Epoch 690/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9021 - val_loss: 35.6009\n",
      "Epoch 691/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8605 - val_loss: 35.6448\n",
      "Epoch 692/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8887 - val_loss: 35.5469\n",
      "Epoch 693/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8774 - val_loss: 35.5431\n",
      "Epoch 694/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8553 - val_loss: 35.4941\n",
      "Epoch 695/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8967 - val_loss: 35.6065\n",
      "Epoch 696/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8774 - val_loss: 35.4824\n",
      "Epoch 697/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8374 - val_loss: 35.5826\n",
      "Epoch 698/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8575 - val_loss: 35.6072\n",
      "Epoch 699/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8433 - val_loss: 35.5355\n",
      "Epoch 700/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8647 - val_loss: 35.5062\n",
      "Epoch 701/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8928 - val_loss: 35.5651\n",
      "Epoch 702/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8559 - val_loss: 35.6768\n",
      "Epoch 703/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8669 - val_loss: 35.6568\n",
      "Epoch 704/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8839 - val_loss: 35.6630\n",
      "Epoch 705/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8605 - val_loss: 35.5906\n",
      "Epoch 706/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8582 - val_loss: 35.6156\n",
      "Epoch 707/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8591 - val_loss: 35.6596\n",
      "Epoch 708/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8878 - val_loss: 35.6511\n",
      "Epoch 709/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8456 - val_loss: 35.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8680 - val_loss: 35.5985\n",
      "Epoch 711/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8616 - val_loss: 35.5598\n",
      "Epoch 712/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8405 - val_loss: 35.5246\n",
      "Epoch 713/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8885 - val_loss: 35.4741\n",
      "Epoch 714/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8708 - val_loss: 35.5285\n",
      "Epoch 715/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8887 - val_loss: 35.4948\n",
      "Epoch 716/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8732 - val_loss: 35.6099\n",
      "Epoch 717/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8576 - val_loss: 35.5293\n",
      "Epoch 718/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9023 - val_loss: 35.4941\n",
      "Epoch 719/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9082 - val_loss: 35.4319\n",
      "Epoch 720/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8819 - val_loss: 35.5772\n",
      "Epoch 721/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8811 - val_loss: 35.6547\n",
      "Epoch 722/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8513 - val_loss: 35.7040\n",
      "Epoch 723/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8489 - val_loss: 35.6076\n",
      "Epoch 724/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8845 - val_loss: 35.6021\n",
      "Epoch 725/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8488 - val_loss: 35.5997\n",
      "Epoch 726/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8601 - val_loss: 35.5043\n",
      "Epoch 727/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8418 - val_loss: 35.5483\n",
      "Epoch 728/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8501 - val_loss: 35.5593\n",
      "Epoch 729/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8384 - val_loss: 35.5368\n",
      "Epoch 730/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8682 - val_loss: 35.5868\n",
      "Epoch 731/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8773 - val_loss: 35.5180\n",
      "Epoch 732/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8510 - val_loss: 35.4798\n",
      "Epoch 733/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8614 - val_loss: 35.5610\n",
      "Epoch 734/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8872 - val_loss: 35.5405\n",
      "Epoch 735/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8693 - val_loss: 35.4815\n",
      "Epoch 736/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8543 - val_loss: 35.5987\n",
      "Epoch 737/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8567 - val_loss: 35.6168\n",
      "Epoch 738/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8872 - val_loss: 35.6616\n",
      "Epoch 739/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8442 - val_loss: 35.7471\n",
      "Epoch 740/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8607 - val_loss: 35.5987\n",
      "Epoch 741/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8493 - val_loss: 35.5929\n",
      "Epoch 742/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8408 - val_loss: 35.7476\n",
      "Epoch 743/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8884 - val_loss: 35.6432\n",
      "Epoch 744/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8437 - val_loss: 35.6730\n",
      "Epoch 745/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8815 - val_loss: 35.6098\n",
      "Epoch 746/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8566 - val_loss: 35.5849\n",
      "Epoch 747/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8570 - val_loss: 35.5639\n",
      "Epoch 748/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8803 - val_loss: 35.4923\n",
      "Epoch 749/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8746 - val_loss: 35.5463\n",
      "Epoch 750/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8681 - val_loss: 35.5080\n",
      "Epoch 751/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8520 - val_loss: 35.5442\n",
      "Epoch 752/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8485 - val_loss: 35.4935\n",
      "Epoch 753/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9033 - val_loss: 35.5943\n",
      "Epoch 754/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8726 - val_loss: 35.5945\n",
      "Epoch 755/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8672 - val_loss: 35.6426\n",
      "Epoch 756/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8556 - val_loss: 35.5718\n",
      "Epoch 757/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8763 - val_loss: 35.6092\n",
      "Epoch 758/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8762 - val_loss: 35.5227\n",
      "Epoch 759/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8809 - val_loss: 35.5034\n",
      "Epoch 760/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8990 - val_loss: 35.5787\n",
      "Epoch 761/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8621 - val_loss: 35.6426\n",
      "Epoch 762/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8715 - val_loss: 35.5955\n",
      "Epoch 763/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8940 - val_loss: 35.4744\n",
      "Epoch 764/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8745 - val_loss: 35.5446\n",
      "Epoch 765/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8508 - val_loss: 35.5329\n",
      "Epoch 766/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8807 - val_loss: 35.5514\n",
      "Epoch 767/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8497 - val_loss: 35.5262\n",
      "Epoch 768/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8774 - val_loss: 35.5593\n",
      "Epoch 769/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8479 - val_loss: 35.5177\n",
      "Epoch 770/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8799 - val_loss: 35.5842\n",
      "Epoch 771/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8643 - val_loss: 35.5850\n",
      "Epoch 772/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8565 - val_loss: 35.6716\n",
      "Epoch 773/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8590 - val_loss: 35.6390\n",
      "Epoch 774/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8720 - val_loss: 35.6022\n",
      "Epoch 775/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8706 - val_loss: 35.5910\n",
      "Epoch 776/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8603 - val_loss: 35.6423\n",
      "Epoch 777/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8742 - val_loss: 35.5967\n",
      "Epoch 778/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8688 - val_loss: 35.5979\n",
      "Epoch 779/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8582 - val_loss: 35.6976\n",
      "Epoch 780/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8854 - val_loss: 35.6288\n",
      "Epoch 781/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8923 - val_loss: 35.7446\n",
      "Epoch 782/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8920 - val_loss: 35.6156\n",
      "Epoch 783/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8745 - val_loss: 35.5968\n",
      "Epoch 784/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8705 - val_loss: 35.6660\n",
      "Epoch 785/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8699 - val_loss: 35.5713\n",
      "Epoch 786/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8623 - val_loss: 35.5273\n",
      "Epoch 787/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8464 - val_loss: 35.5731\n",
      "Epoch 788/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8524 - val_loss: 35.6337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 789/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8896 - val_loss: 35.5909\n",
      "Epoch 790/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8546 - val_loss: 35.5278\n",
      "Epoch 791/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8579 - val_loss: 35.5770\n",
      "Epoch 792/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8716 - val_loss: 35.4964\n",
      "Epoch 793/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8483 - val_loss: 35.5234\n",
      "Epoch 794/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8712 - val_loss: 35.5128\n",
      "Epoch 795/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8874 - val_loss: 35.6443\n",
      "Epoch 796/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8665 - val_loss: 35.5265\n",
      "Epoch 797/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8500 - val_loss: 35.6704\n",
      "Epoch 798/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8763 - val_loss: 35.7236\n",
      "Epoch 799/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8691 - val_loss: 35.6567\n",
      "Epoch 800/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8640 - val_loss: 35.5395\n",
      "Epoch 801/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8675 - val_loss: 35.7003\n",
      "Epoch 802/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9126 - val_loss: 35.6586\n",
      "Epoch 803/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8889 - val_loss: 35.5294\n",
      "Epoch 804/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8542 - val_loss: 35.6054\n",
      "Epoch 805/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8993 - val_loss: 35.6834\n",
      "Epoch 806/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8806 - val_loss: 35.6470\n",
      "Epoch 807/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8493 - val_loss: 35.6374\n",
      "Epoch 808/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8506 - val_loss: 35.7020\n",
      "Epoch 809/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8733 - val_loss: 35.6767\n",
      "Epoch 810/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8554 - val_loss: 35.6249\n",
      "Epoch 811/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8577 - val_loss: 35.4861\n",
      "Epoch 812/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8935 - val_loss: 35.5846\n",
      "Epoch 813/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8637 - val_loss: 35.5995\n",
      "Epoch 814/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8924 - val_loss: 35.6571\n",
      "Epoch 815/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8606 - val_loss: 35.6678\n",
      "Epoch 816/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8611 - val_loss: 35.5094\n",
      "Epoch 817/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8967 - val_loss: 35.5941\n",
      "Epoch 818/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9120 - val_loss: 35.5596\n",
      "Epoch 819/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8999 - val_loss: 35.6486\n",
      "Epoch 820/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8619 - val_loss: 35.5246\n",
      "Epoch 821/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8393 - val_loss: 35.4516\n",
      "Epoch 822/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8720 - val_loss: 35.5802\n",
      "Epoch 823/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8757 - val_loss: 35.5260\n",
      "Epoch 824/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8813 - val_loss: 35.6514\n",
      "Epoch 825/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8785 - val_loss: 35.6458\n",
      "Epoch 826/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.9057 - val_loss: 35.4799\n",
      "Epoch 827/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8723 - val_loss: 35.4913\n",
      "Epoch 828/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8508 - val_loss: 35.4487\n",
      "Epoch 829/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8823 - val_loss: 35.4959\n",
      "Epoch 830/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8523 - val_loss: 35.5631\n",
      "Epoch 831/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8460 - val_loss: 35.7035\n",
      "Epoch 832/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8717 - val_loss: 35.6698\n",
      "Epoch 833/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8819 - val_loss: 35.6059\n",
      "Epoch 834/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8559 - val_loss: 35.5564\n",
      "Epoch 835/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8716 - val_loss: 35.6009\n",
      "Epoch 836/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8607 - val_loss: 35.6300\n",
      "Epoch 837/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8572 - val_loss: 35.6447\n",
      "Epoch 838/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8893 - val_loss: 35.5722\n",
      "Epoch 839/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8427 - val_loss: 35.5709\n",
      "Epoch 840/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8464 - val_loss: 35.5385\n",
      "Epoch 841/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8546 - val_loss: 35.6410\n",
      "Epoch 842/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8514 - val_loss: 35.5364\n",
      "Epoch 843/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8571 - val_loss: 35.6129\n",
      "Epoch 844/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8704 - val_loss: 35.5384\n",
      "Epoch 845/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8598 - val_loss: 35.5810\n",
      "Epoch 846/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8669 - val_loss: 35.5926\n",
      "Epoch 847/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8521 - val_loss: 35.5923\n",
      "Epoch 848/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8735 - val_loss: 35.6214\n",
      "Epoch 849/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8772 - val_loss: 35.5309\n",
      "Epoch 850/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8735 - val_loss: 35.4833\n",
      "Epoch 851/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8580 - val_loss: 35.5947\n",
      "Epoch 852/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8445 - val_loss: 35.5569\n",
      "Epoch 853/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.9004 - val_loss: 35.6673\n",
      "Epoch 854/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8780 - val_loss: 35.5992\n",
      "Epoch 855/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8610 - val_loss: 35.5648\n",
      "Epoch 856/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8900 - val_loss: 35.6331\n",
      "Epoch 857/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8545 - val_loss: 35.5644\n",
      "Epoch 858/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8895 - val_loss: 35.5944\n",
      "Epoch 859/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8608 - val_loss: 35.6199\n",
      "Epoch 860/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8555 - val_loss: 35.6274\n",
      "Epoch 861/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8537 - val_loss: 35.6656\n",
      "Epoch 862/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8629 - val_loss: 35.5848\n",
      "Epoch 863/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 30.8813 - val_loss: 35.5862\n",
      "Epoch 864/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8743 - val_loss: 35.5266\n",
      "Epoch 865/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9002 - val_loss: 35.7133\n",
      "Epoch 866/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8706 - val_loss: 35.5645\n",
      "Epoch 867/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9131 - val_loss: 35.5749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 868/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8898 - val_loss: 35.5241\n",
      "Epoch 869/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8862 - val_loss: 35.5432\n",
      "Epoch 870/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8427 - val_loss: 35.5804\n",
      "Epoch 871/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8500 - val_loss: 35.6093\n",
      "Epoch 872/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.8794 - val_loss: 35.6082\n",
      "Epoch 873/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8620 - val_loss: 35.5350\n",
      "Epoch 874/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8794 - val_loss: 35.5336\n",
      "Epoch 875/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9158 - val_loss: 35.5653\n",
      "Epoch 876/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8658 - val_loss: 35.5948\n",
      "Epoch 877/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8511 - val_loss: 35.7638\n",
      "Epoch 878/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8535 - val_loss: 35.8351\n",
      "Epoch 879/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8669 - val_loss: 35.6756\n",
      "Epoch 880/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9319 - val_loss: 35.5650\n",
      "Epoch 881/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8650 - val_loss: 35.5501\n",
      "Epoch 882/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8793 - val_loss: 35.5399\n",
      "Epoch 883/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8509 - val_loss: 35.5721\n",
      "Epoch 884/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8758 - val_loss: 35.6943\n",
      "Epoch 885/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8992 - val_loss: 35.7421\n",
      "Epoch 886/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8695 - val_loss: 35.7066\n",
      "Epoch 887/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8815 - val_loss: 35.7941\n",
      "Epoch 888/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8662 - val_loss: 35.6633\n",
      "Epoch 889/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8555 - val_loss: 35.6055\n",
      "Epoch 890/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8866 - val_loss: 35.6227\n",
      "Epoch 891/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8586 - val_loss: 35.7029\n",
      "Epoch 892/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8707 - val_loss: 35.6327\n",
      "Epoch 893/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8918 - val_loss: 35.6471\n",
      "Epoch 894/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8483 - val_loss: 35.5258\n",
      "Epoch 895/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8597 - val_loss: 35.5687\n",
      "Epoch 896/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8739 - val_loss: 35.5524\n",
      "Epoch 897/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8555 - val_loss: 35.5952\n",
      "Epoch 898/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8648 - val_loss: 35.5439\n",
      "Epoch 899/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8860 - val_loss: 35.6147\n",
      "Epoch 900/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8749 - val_loss: 35.6179\n",
      "Epoch 901/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8681 - val_loss: 35.6845\n",
      "Epoch 902/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8537 - val_loss: 35.5992\n",
      "Epoch 903/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8435 - val_loss: 35.5695\n",
      "Epoch 904/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8518 - val_loss: 35.5595\n",
      "Epoch 905/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8583 - val_loss: 35.6124\n",
      "Epoch 906/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8859 - val_loss: 35.6321\n",
      "Epoch 907/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8711 - val_loss: 35.6114\n",
      "Epoch 908/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8493 - val_loss: 35.5868\n",
      "Epoch 909/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8479 - val_loss: 35.5882\n",
      "Epoch 910/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8815 - val_loss: 35.5230\n",
      "Epoch 911/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8632 - val_loss: 35.6055\n",
      "Epoch 912/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8815 - val_loss: 35.5784\n",
      "Epoch 913/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8724 - val_loss: 35.5901\n",
      "Epoch 914/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8661 - val_loss: 35.5180\n",
      "Epoch 915/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8829 - val_loss: 35.4709\n",
      "Epoch 916/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8951 - val_loss: 35.4895\n",
      "Epoch 917/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8913 - val_loss: 35.5179\n",
      "Epoch 918/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8681 - val_loss: 35.5807\n",
      "Epoch 919/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9082 - val_loss: 35.6287\n",
      "Epoch 920/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8791 - val_loss: 35.5805\n",
      "Epoch 921/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8714 - val_loss: 35.6006\n",
      "Epoch 922/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8692 - val_loss: 35.5067\n",
      "Epoch 923/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8441 - val_loss: 35.5119\n",
      "Epoch 924/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8384 - val_loss: 35.6172\n",
      "Epoch 925/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8688 - val_loss: 35.5560\n",
      "Epoch 926/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8606 - val_loss: 35.5352\n",
      "Epoch 927/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8841 - val_loss: 35.5819\n",
      "Epoch 928/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8966 - val_loss: 35.7686\n",
      "Epoch 929/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9355 - val_loss: 35.6575\n",
      "Epoch 930/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8764 - val_loss: 35.5585\n",
      "Epoch 931/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8696 - val_loss: 35.7404\n",
      "Epoch 932/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8650 - val_loss: 35.6874\n",
      "Epoch 933/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8590 - val_loss: 35.6320\n",
      "Epoch 934/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8736 - val_loss: 35.5711\n",
      "Epoch 935/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8882 - val_loss: 35.5885\n",
      "Epoch 936/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9176 - val_loss: 35.7294\n",
      "Epoch 937/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8642 - val_loss: 35.6547\n",
      "Epoch 938/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8708 - val_loss: 35.6441\n",
      "Epoch 939/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8478 - val_loss: 35.5518\n",
      "Epoch 940/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8658 - val_loss: 35.5615\n",
      "Epoch 941/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8560 - val_loss: 35.6107\n",
      "Epoch 942/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8736 - val_loss: 35.7279\n",
      "Epoch 943/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9179 - val_loss: 35.5756\n",
      "Epoch 944/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8654 - val_loss: 35.5530\n",
      "Epoch 945/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8781 - val_loss: 35.6321\n",
      "Epoch 946/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8724 - val_loss: 35.5913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9450 - val_loss: 35.5770\n",
      "Epoch 948/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8601 - val_loss: 35.6581\n",
      "Epoch 949/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8635 - val_loss: 35.6832\n",
      "Epoch 950/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8889 - val_loss: 35.7555\n",
      "Epoch 951/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8686 - val_loss: 35.5988\n",
      "Epoch 952/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8589 - val_loss: 35.5829\n",
      "Epoch 953/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8489 - val_loss: 35.6916\n",
      "Epoch 954/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8964 - val_loss: 35.6285\n",
      "Epoch 955/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8970 - val_loss: 35.6889\n",
      "Epoch 956/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8430 - val_loss: 35.6076\n",
      "Epoch 957/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8540 - val_loss: 35.6575\n",
      "Epoch 958/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9228 - val_loss: 35.7065\n",
      "Epoch 959/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8521 - val_loss: 35.7206\n",
      "Epoch 960/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8718 - val_loss: 35.6202\n",
      "Epoch 961/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8503 - val_loss: 35.5660\n",
      "Epoch 962/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8739 - val_loss: 35.5033\n",
      "Epoch 963/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8607 - val_loss: 35.4833\n",
      "Epoch 964/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8992 - val_loss: 35.5574\n",
      "Epoch 965/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8582 - val_loss: 35.4888\n",
      "Epoch 966/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8565 - val_loss: 35.5373\n",
      "Epoch 967/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8791 - val_loss: 35.4216\n",
      "Epoch 968/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8543 - val_loss: 35.4644\n",
      "Epoch 969/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8829 - val_loss: 35.6914\n",
      "Epoch 970/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8434 - val_loss: 35.5155\n",
      "Epoch 971/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8411 - val_loss: 35.5432\n",
      "Epoch 972/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8577 - val_loss: 35.5549\n",
      "Epoch 973/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8632 - val_loss: 35.6172\n",
      "Epoch 974/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8744 - val_loss: 35.5310\n",
      "Epoch 975/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8908 - val_loss: 35.6096\n",
      "Epoch 976/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9151 - val_loss: 35.6362\n",
      "Epoch 977/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8695 - val_loss: 35.5243\n",
      "Epoch 978/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8554 - val_loss: 35.5792\n",
      "Epoch 979/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8696 - val_loss: 35.4999\n",
      "Epoch 980/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8601 - val_loss: 35.5325\n",
      "Epoch 981/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8749 - val_loss: 35.5868\n",
      "Epoch 982/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8632 - val_loss: 35.6337\n",
      "Epoch 983/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8619 - val_loss: 35.5377\n",
      "Epoch 984/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8812 - val_loss: 35.4729\n",
      "Epoch 985/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8576 - val_loss: 35.5138\n",
      "Epoch 986/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.9154 - val_loss: 35.5688\n",
      "Epoch 987/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8464 - val_loss: 35.6923\n",
      "Epoch 988/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8808 - val_loss: 35.5812\n",
      "Epoch 989/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8791 - val_loss: 35.4475\n",
      "Epoch 990/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9058 - val_loss: 35.4424\n",
      "Epoch 991/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8487 - val_loss: 35.6301\n",
      "Epoch 992/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 30.8457 - val_loss: 35.7152\n",
      "Epoch 993/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8802 - val_loss: 35.6303\n",
      "Epoch 994/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8684 - val_loss: 35.5882\n",
      "Epoch 995/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8732 - val_loss: 35.5652\n",
      "Epoch 996/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8778 - val_loss: 35.6530\n",
      "Epoch 997/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8786 - val_loss: 35.6929\n",
      "Epoch 998/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.9123 - val_loss: 35.6880\n",
      "Epoch 999/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8424 - val_loss: 35.6127\n",
      "Epoch 1000/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.8474 - val_loss: 35.5182\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(32,activation='relu',input_shape=(x_train.shape[1],)))\n",
    "model1.add(Dense(16,activation='relu'))\n",
    "model1.add(Dense(1))\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "model1.compile(optimizer=opt,loss='MAPE')\n",
    "history1 = model1.fit(x=x_train,y=y_train,epochs=1000,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82cfebe",
   "metadata": {
    "id": "c82cfebe"
   },
   "source": [
    "### MODEL 2 - Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f83f0",
   "metadata": {
    "collapsed": true,
    "id": "785f83f0",
    "outputId": "87222c29-baa3-4e1b-9ce3-bc992f967404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "13/13 [==============================] - 1s 13ms/step - loss: 99.5559 - val_loss: 95.2041\n",
      "Epoch 2/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 91.8190 - val_loss: 86.2072\n",
      "Epoch 3/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 82.2448 - val_loss: 75.0067\n",
      "Epoch 4/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 70.3085 - val_loss: 63.6074\n",
      "Epoch 5/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 57.2590 - val_loss: 55.3847\n",
      "Epoch 6/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 48.2394 - val_loss: 50.6813\n",
      "Epoch 7/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 42.9846 - val_loss: 47.2100\n",
      "Epoch 8/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 40.0991 - val_loss: 45.2596\n",
      "Epoch 9/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 38.3169 - val_loss: 43.7570\n",
      "Epoch 10/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 36.7577 - val_loss: 42.4340\n",
      "Epoch 11/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 35.3923 - val_loss: 41.2075\n",
      "Epoch 12/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 34.2631 - val_loss: 40.0688\n",
      "Epoch 13/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 33.1654 - val_loss: 38.8756\n",
      "Epoch 14/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 32.2216 - val_loss: 37.7522\n",
      "Epoch 15/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.2965 - val_loss: 36.7284\n",
      "Epoch 16/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.4219 - val_loss: 35.7842\n",
      "Epoch 17/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 29.5800 - val_loss: 34.9210\n",
      "Epoch 18/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 28.9251 - val_loss: 34.1877\n",
      "Epoch 19/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 28.4433 - val_loss: 33.7503\n",
      "Epoch 20/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 28.0894 - val_loss: 33.2419\n",
      "Epoch 21/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 27.7754 - val_loss: 32.7740\n",
      "Epoch 22/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 27.5166 - val_loss: 32.3858\n",
      "Epoch 23/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 27.2573 - val_loss: 32.0107\n",
      "Epoch 24/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.9743 - val_loss: 31.6707\n",
      "Epoch 25/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 26.7922 - val_loss: 31.3932\n",
      "Epoch 26/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 26.6355 - val_loss: 31.1597\n",
      "Epoch 27/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 26.4624 - val_loss: 30.8905\n",
      "Epoch 28/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.3036 - val_loss: 30.8356\n",
      "Epoch 29/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 26.1154 - val_loss: 30.5270\n",
      "Epoch 30/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 25.9733 - val_loss: 30.3476\n",
      "Epoch 31/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 25.8339 - val_loss: 30.2485\n",
      "Epoch 32/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.7130 - val_loss: 30.0589\n",
      "Epoch 33/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 25.5743 - val_loss: 29.9156\n",
      "Epoch 34/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 25.4520 - val_loss: 29.8468\n",
      "Epoch 35/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 25.3095 - val_loss: 29.6502\n",
      "Epoch 36/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 25.1868 - val_loss: 29.5666\n",
      "Epoch 37/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 25.1028 - val_loss: 29.4813\n",
      "Epoch 38/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.9149 - val_loss: 29.3768\n",
      "Epoch 39/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.7599 - val_loss: 29.2144\n",
      "Epoch 40/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.6495 - val_loss: 29.1434\n",
      "Epoch 41/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.5740 - val_loss: 29.0065\n",
      "Epoch 42/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.4402 - val_loss: 28.8387\n",
      "Epoch 43/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.3373 - val_loss: 28.7320\n",
      "Epoch 44/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.2677 - val_loss: 28.6377\n",
      "Epoch 45/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.1975 - val_loss: 28.5143\n",
      "Epoch 46/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.0390 - val_loss: 28.4026\n",
      "Epoch 47/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.9296 - val_loss: 28.3112\n",
      "Epoch 48/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.8720 - val_loss: 28.1906\n",
      "Epoch 49/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.7392 - val_loss: 28.0842\n",
      "Epoch 50/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.6822 - val_loss: 28.0028\n",
      "Epoch 51/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.6155 - val_loss: 27.9102\n",
      "Epoch 52/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.5610 - val_loss: 27.8061\n",
      "Epoch 53/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.4562 - val_loss: 27.7713\n",
      "Epoch 54/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.3420 - val_loss: 27.6236\n",
      "Epoch 55/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.3031 - val_loss: 27.5581\n",
      "Epoch 56/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.2284 - val_loss: 27.4753\n",
      "Epoch 57/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.0750 - val_loss: 27.3894\n",
      "Epoch 58/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.0401 - val_loss: 27.3283\n",
      "Epoch 59/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.9195 - val_loss: 27.2441\n",
      "Epoch 60/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.8577 - val_loss: 27.1874\n",
      "Epoch 61/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.7693 - val_loss: 27.0733\n",
      "Epoch 62/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.6977 - val_loss: 27.0185\n",
      "Epoch 63/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.6492 - val_loss: 27.0310\n",
      "Epoch 64/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.5778 - val_loss: 26.9179\n",
      "Epoch 65/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.5250 - val_loss: 26.8342\n",
      "Epoch 66/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.4138 - val_loss: 26.7947\n",
      "Epoch 67/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.3456 - val_loss: 26.8040\n",
      "Epoch 68/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 22.3802 - val_loss: 26.6192\n",
      "Epoch 69/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.2231 - val_loss: 26.5775\n",
      "Epoch 70/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.1505 - val_loss: 26.4904\n",
      "Epoch 71/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.0506 - val_loss: 26.3733\n",
      "Epoch 72/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.0326 - val_loss: 26.3283\n",
      "Epoch 73/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.9951 - val_loss: 26.3460\n",
      "Epoch 74/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.9984 - val_loss: 26.3484\n",
      "Epoch 75/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.8611 - val_loss: 26.1329\n",
      "Epoch 76/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.7803 - val_loss: 26.1748\n",
      "Epoch 77/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.7768 - val_loss: 26.0875\n",
      "Epoch 78/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.6589 - val_loss: 26.0304\n",
      "Epoch 79/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.6248 - val_loss: 26.1415\n",
      "Epoch 80/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.5664 - val_loss: 25.9250\n",
      "Epoch 81/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.4956 - val_loss: 25.8145\n",
      "Epoch 82/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.4228 - val_loss: 25.8017\n",
      "Epoch 83/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 21.3820 - val_loss: 25.8008\n",
      "Epoch 84/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.3447 - val_loss: 25.6501\n",
      "Epoch 85/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.3263 - val_loss: 25.7360\n",
      "Epoch 86/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.2566 - val_loss: 25.5891\n",
      "Epoch 87/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.1581 - val_loss: 25.6659\n",
      "Epoch 88/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.2343 - val_loss: 25.5382\n",
      "Epoch 89/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.1375 - val_loss: 25.5724\n",
      "Epoch 90/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.1559 - val_loss: 25.5702\n",
      "Epoch 91/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.0483 - val_loss: 25.5173\n",
      "Epoch 92/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.0036 - val_loss: 25.2746\n",
      "Epoch 93/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.9588 - val_loss: 25.5092\n",
      "Epoch 94/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.9547 - val_loss: 25.6247\n",
      "Epoch 95/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.0112 - val_loss: 25.3644\n",
      "Epoch 96/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.8950 - val_loss: 25.3068\n",
      "Epoch 97/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.8783 - val_loss: 25.1057\n",
      "Epoch 98/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.8936 - val_loss: 25.1569\n",
      "Epoch 99/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.8282 - val_loss: 25.1821\n",
      "Epoch 100/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.8057 - val_loss: 25.1401\n",
      "Epoch 101/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.7598 - val_loss: 25.0972\n",
      "Epoch 102/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.7294 - val_loss: 25.2362\n",
      "Epoch 103/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.8064 - val_loss: 25.2272\n",
      "Epoch 104/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.8562 - val_loss: 25.2404\n",
      "Epoch 105/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.7598 - val_loss: 25.1553\n",
      "Epoch 106/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6728 - val_loss: 25.3089\n",
      "Epoch 107/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6759 - val_loss: 25.3347\n",
      "Epoch 108/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6485 - val_loss: 25.0330\n",
      "Epoch 109/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5959 - val_loss: 25.0644\n",
      "Epoch 110/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5940 - val_loss: 25.0676\n",
      "Epoch 111/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5740 - val_loss: 25.0639\n",
      "Epoch 112/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5357 - val_loss: 25.0350\n",
      "Epoch 113/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5459 - val_loss: 25.0315\n",
      "Epoch 114/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5236 - val_loss: 25.3474\n",
      "Epoch 115/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.4796 - val_loss: 24.9979\n",
      "Epoch 116/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.7024 - val_loss: 25.0559\n",
      "Epoch 117/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5474 - val_loss: 25.1366\n",
      "Epoch 118/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.4914 - val_loss: 24.9971\n",
      "Epoch 119/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.5118 - val_loss: 25.0135\n",
      "Epoch 120/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3386 - val_loss: 25.2709\n",
      "Epoch 121/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.4410 - val_loss: 25.1215\n",
      "Epoch 122/1000\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 20.4354 - val_loss: 24.9111\n",
      "Epoch 123/1000\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 20.4502 - val_loss: 24.9328\n",
      "Epoch 124/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.4606 - val_loss: 24.9162\n",
      "Epoch 125/1000\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 20.3924 - val_loss: 25.0252\n",
      "Epoch 126/1000\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.4413 - val_loss: 24.9554\n",
      "Epoch 127/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3140 - val_loss: 24.8796\n",
      "Epoch 128/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3456 - val_loss: 25.0958\n",
      "Epoch 129/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.4045 - val_loss: 24.9319\n",
      "Epoch 130/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2621 - val_loss: 24.8830\n",
      "Epoch 131/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3003 - val_loss: 24.9812\n",
      "Epoch 132/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3925 - val_loss: 24.9134\n",
      "Epoch 133/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2566 - val_loss: 24.9259\n",
      "Epoch 134/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3061 - val_loss: 24.8792\n",
      "Epoch 135/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2561 - val_loss: 25.1539\n",
      "Epoch 136/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2500 - val_loss: 24.8585\n",
      "Epoch 137/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2145 - val_loss: 24.8398\n",
      "Epoch 138/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2555 - val_loss: 24.8673\n",
      "Epoch 139/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2526 - val_loss: 24.8126\n",
      "Epoch 140/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2753 - val_loss: 24.7145\n",
      "Epoch 141/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2007 - val_loss: 24.7156\n",
      "Epoch 142/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1864 - val_loss: 24.7508\n",
      "Epoch 143/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0985 - val_loss: 24.9327\n",
      "Epoch 144/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2088 - val_loss: 24.8024\n",
      "Epoch 145/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2430 - val_loss: 24.9204\n",
      "Epoch 146/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1860 - val_loss: 25.0574\n",
      "Epoch 147/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1324 - val_loss: 24.6755\n",
      "Epoch 148/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1301 - val_loss: 24.7581\n",
      "Epoch 149/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1888 - val_loss: 24.7065\n",
      "Epoch 150/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1531 - val_loss: 24.7311\n",
      "Epoch 151/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2096 - val_loss: 24.6346\n",
      "Epoch 152/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1099 - val_loss: 24.7985\n",
      "Epoch 153/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0438 - val_loss: 24.6806\n",
      "Epoch 154/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1375 - val_loss: 24.6528\n",
      "Epoch 155/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1353 - val_loss: 24.6643\n",
      "Epoch 156/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1042 - val_loss: 24.7543\n",
      "Epoch 157/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1483 - val_loss: 24.6260\n",
      "Epoch 158/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0499 - val_loss: 24.6512\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0562 - val_loss: 24.7235\n",
      "Epoch 160/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0572 - val_loss: 24.7400\n",
      "Epoch 161/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1147 - val_loss: 24.6927\n",
      "Epoch 162/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1488 - val_loss: 24.6829\n",
      "Epoch 163/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0648 - val_loss: 24.7523\n",
      "Epoch 164/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1678 - val_loss: 24.6222\n",
      "Epoch 165/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0385 - val_loss: 24.5923\n",
      "Epoch 166/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0584 - val_loss: 24.6451\n",
      "Epoch 167/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0774 - val_loss: 24.5883\n",
      "Epoch 168/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1531 - val_loss: 24.5807\n",
      "Epoch 169/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0534 - val_loss: 25.0348\n",
      "Epoch 170/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0746 - val_loss: 24.6365\n",
      "Epoch 171/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0018 - val_loss: 24.7318\n",
      "Epoch 172/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0627 - val_loss: 24.5612\n",
      "Epoch 173/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1027 - val_loss: 24.6169\n",
      "Epoch 174/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0552 - val_loss: 24.5809\n",
      "Epoch 175/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9722 - val_loss: 24.5570\n",
      "Epoch 176/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0496 - val_loss: 24.7036\n",
      "Epoch 177/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9716 - val_loss: 24.6148\n",
      "Epoch 178/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0063 - val_loss: 24.5229\n",
      "Epoch 179/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.8637 - val_loss: 24.6775\n",
      "Epoch 180/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9700 - val_loss: 24.4766\n",
      "Epoch 181/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0227 - val_loss: 24.5423\n",
      "Epoch 182/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9335 - val_loss: 24.5652\n",
      "Epoch 183/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9991 - val_loss: 24.5087\n",
      "Epoch 184/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9104 - val_loss: 24.5779\n",
      "Epoch 185/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9784 - val_loss: 24.5783\n",
      "Epoch 186/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9362 - val_loss: 24.5673\n",
      "Epoch 187/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9410 - val_loss: 24.5261\n",
      "Epoch 188/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0705 - val_loss: 24.5007\n",
      "Epoch 189/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8871 - val_loss: 24.5450\n",
      "Epoch 190/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9408 - val_loss: 24.5294\n",
      "Epoch 191/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.8961 - val_loss: 24.5814\n",
      "Epoch 192/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8905 - val_loss: 24.5491\n",
      "Epoch 193/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9302 - val_loss: 24.6499\n",
      "Epoch 194/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8893 - val_loss: 24.4842\n",
      "Epoch 195/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8956 - val_loss: 24.4734\n",
      "Epoch 196/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8236 - val_loss: 24.6282\n",
      "Epoch 197/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.8579 - val_loss: 24.4537\n",
      "Epoch 198/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.8384 - val_loss: 24.5936\n",
      "Epoch 199/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.8467 - val_loss: 24.4613\n",
      "Epoch 200/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8507 - val_loss: 24.7955\n",
      "Epoch 201/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9615 - val_loss: 24.5558\n",
      "Epoch 202/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8961 - val_loss: 24.4712\n",
      "Epoch 203/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8224 - val_loss: 24.4894\n",
      "Epoch 204/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8281 - val_loss: 24.6160\n",
      "Epoch 205/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8599 - val_loss: 24.4739\n",
      "Epoch 206/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.8761 - val_loss: 24.4682\n",
      "Epoch 207/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7887 - val_loss: 24.5182\n",
      "Epoch 208/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7918 - val_loss: 24.5641\n",
      "Epoch 209/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7994 - val_loss: 24.4576\n",
      "Epoch 210/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7520 - val_loss: 24.6625\n",
      "Epoch 211/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7557 - val_loss: 24.4191\n",
      "Epoch 212/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.7395 - val_loss: 24.4153\n",
      "Epoch 213/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7453 - val_loss: 24.3895\n",
      "Epoch 214/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.7293 - val_loss: 24.5893\n",
      "Epoch 215/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8396 - val_loss: 24.4685\n",
      "Epoch 216/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7068 - val_loss: 24.3651\n",
      "Epoch 217/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7331 - val_loss: 24.3881\n",
      "Epoch 218/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.8054 - val_loss: 24.5173\n",
      "Epoch 219/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7806 - val_loss: 24.4329\n",
      "Epoch 220/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.7463 - val_loss: 24.6483\n",
      "Epoch 221/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7434 - val_loss: 24.4152\n",
      "Epoch 222/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7993 - val_loss: 24.3590\n",
      "Epoch 223/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.7380 - val_loss: 24.5425\n",
      "Epoch 224/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7306 - val_loss: 24.4753\n",
      "Epoch 225/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7275 - val_loss: 24.3004\n",
      "Epoch 226/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7039 - val_loss: 24.3701\n",
      "Epoch 227/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.7714 - val_loss: 24.4737\n",
      "Epoch 228/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6370 - val_loss: 24.5565\n",
      "Epoch 229/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7206 - val_loss: 24.3259\n",
      "Epoch 230/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7330 - val_loss: 24.3706\n",
      "Epoch 231/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6160 - val_loss: 24.3337\n",
      "Epoch 232/1000\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 19.5969 - val_loss: 24.7342\n",
      "Epoch 233/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7242 - val_loss: 24.4779\n",
      "Epoch 234/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6685 - val_loss: 24.3667\n",
      "Epoch 235/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7086 - val_loss: 24.4156\n",
      "Epoch 236/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6209 - val_loss: 24.3807\n",
      "Epoch 237/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6702 - val_loss: 24.6947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.7376 - val_loss: 24.6741\n",
      "Epoch 239/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6731 - val_loss: 24.3250\n",
      "Epoch 240/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.7182 - val_loss: 24.2961\n",
      "Epoch 241/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5740 - val_loss: 24.3500\n",
      "Epoch 242/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6820 - val_loss: 24.4054\n",
      "Epoch 243/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6501 - val_loss: 24.4375\n",
      "Epoch 244/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5314 - val_loss: 24.6045\n",
      "Epoch 245/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6467 - val_loss: 24.3853\n",
      "Epoch 246/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.5867 - val_loss: 24.4360\n",
      "Epoch 247/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.6549 - val_loss: 24.2896\n",
      "Epoch 248/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.6868 - val_loss: 24.3265\n",
      "Epoch 249/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.6451 - val_loss: 24.2785\n",
      "Epoch 250/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5346 - val_loss: 24.4145\n",
      "Epoch 251/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6643 - val_loss: 24.2814\n",
      "Epoch 252/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.6237 - val_loss: 24.2873\n",
      "Epoch 253/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5570 - val_loss: 24.3062\n",
      "Epoch 254/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5723 - val_loss: 24.1835\n",
      "Epoch 255/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6441 - val_loss: 24.3513\n",
      "Epoch 256/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6528 - val_loss: 24.2619\n",
      "Epoch 257/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5763 - val_loss: 24.3078\n",
      "Epoch 258/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5376 - val_loss: 24.2253\n",
      "Epoch 259/1000\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 19.4829 - val_loss: 24.4097\n",
      "Epoch 260/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.6880 - val_loss: 24.3032\n",
      "Epoch 261/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4831 - val_loss: 24.2289\n",
      "Epoch 262/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5842 - val_loss: 24.3223\n",
      "Epoch 263/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.5195 - val_loss: 24.2014\n",
      "Epoch 264/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4896 - val_loss: 24.4137\n",
      "Epoch 265/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5408 - val_loss: 24.2756\n",
      "Epoch 266/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4702 - val_loss: 24.2933\n",
      "Epoch 267/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.5046 - val_loss: 24.4241\n",
      "Epoch 268/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5080 - val_loss: 24.2497\n",
      "Epoch 269/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5344 - val_loss: 24.3739\n",
      "Epoch 270/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4656 - val_loss: 24.2778\n",
      "Epoch 271/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4747 - val_loss: 24.3045\n",
      "Epoch 272/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4943 - val_loss: 24.2597\n",
      "Epoch 273/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5019 - val_loss: 24.4927\n",
      "Epoch 274/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.5789 - val_loss: 24.2897\n",
      "Epoch 275/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4562 - val_loss: 24.2826\n",
      "Epoch 276/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4507 - val_loss: 24.2736\n",
      "Epoch 277/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4500 - val_loss: 24.2301\n",
      "Epoch 278/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4077 - val_loss: 24.2026\n",
      "Epoch 279/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.5527 - val_loss: 24.1797\n",
      "Epoch 280/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4706 - val_loss: 24.1342\n",
      "Epoch 281/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5792 - val_loss: 24.1401\n",
      "Epoch 282/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4609 - val_loss: 24.1311\n",
      "Epoch 283/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4177 - val_loss: 24.0953\n",
      "Epoch 284/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4915 - val_loss: 24.0954\n",
      "Epoch 285/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4136 - val_loss: 24.1631\n",
      "Epoch 286/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4075 - val_loss: 24.1706\n",
      "Epoch 287/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.3866 - val_loss: 24.2445\n",
      "Epoch 288/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4897 - val_loss: 24.1419\n",
      "Epoch 289/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.3396 - val_loss: 24.2955\n",
      "Epoch 290/1000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.3804 - val_loss: 24.2138\n",
      "Epoch 291/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4805 - val_loss: 24.1361\n",
      "Epoch 292/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.5305 - val_loss: 24.3176\n",
      "Epoch 293/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3859 - val_loss: 24.3063\n",
      "Epoch 294/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4165 - val_loss: 24.1812\n",
      "Epoch 295/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3628 - val_loss: 24.1789\n",
      "Epoch 296/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3780 - val_loss: 24.1703\n",
      "Epoch 297/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3024 - val_loss: 24.2304\n",
      "Epoch 298/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3747 - val_loss: 24.1635\n",
      "Epoch 299/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3101 - val_loss: 24.2466\n",
      "Epoch 300/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4364 - val_loss: 24.1246\n",
      "Epoch 301/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4538 - val_loss: 24.0981\n",
      "Epoch 302/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4315 - val_loss: 24.0188\n",
      "Epoch 303/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2835 - val_loss: 24.0435\n",
      "Epoch 304/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.4299 - val_loss: 24.0845\n",
      "Epoch 305/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.3297 - val_loss: 24.1000\n",
      "Epoch 306/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2927 - val_loss: 24.2217\n",
      "Epoch 307/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4465 - val_loss: 24.1124\n",
      "Epoch 308/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3713 - val_loss: 24.1394\n",
      "Epoch 309/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3024 - val_loss: 24.0650\n",
      "Epoch 310/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3061 - val_loss: 24.0703\n",
      "Epoch 311/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3744 - val_loss: 24.0903\n",
      "Epoch 312/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3926 - val_loss: 24.1190\n",
      "Epoch 313/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2917 - val_loss: 24.0911\n",
      "Epoch 314/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.3720 - val_loss: 24.0341\n",
      "Epoch 315/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2967 - val_loss: 24.1973\n",
      "Epoch 316/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3107 - val_loss: 24.0849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2973 - val_loss: 24.0571\n",
      "Epoch 318/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2024 - val_loss: 24.3514\n",
      "Epoch 319/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2863 - val_loss: 24.0761\n",
      "Epoch 320/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.4778 - val_loss: 24.1323\n",
      "Epoch 321/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2092 - val_loss: 24.1110\n",
      "Epoch 322/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2640 - val_loss: 24.1559\n",
      "Epoch 323/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2643 - val_loss: 24.1191\n",
      "Epoch 324/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2625 - val_loss: 24.1046\n",
      "Epoch 325/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.3276 - val_loss: 24.1135\n",
      "Epoch 326/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2999 - val_loss: 24.0455\n",
      "Epoch 327/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.3691 - val_loss: 24.1259\n",
      "Epoch 328/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1338 - val_loss: 24.0028\n",
      "Epoch 329/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2574 - val_loss: 24.0168\n",
      "Epoch 330/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2425 - val_loss: 24.0964\n",
      "Epoch 331/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2139 - val_loss: 24.0934\n",
      "Epoch 332/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2711 - val_loss: 24.0538\n",
      "Epoch 333/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2805 - val_loss: 24.1228\n",
      "Epoch 334/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2896 - val_loss: 24.0449\n",
      "Epoch 335/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1873 - val_loss: 24.0587\n",
      "Epoch 336/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1557 - val_loss: 24.1479\n",
      "Epoch 337/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2336 - val_loss: 23.9920\n",
      "Epoch 338/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2153 - val_loss: 24.1207\n",
      "Epoch 339/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1684 - val_loss: 24.0616\n",
      "Epoch 340/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1765 - val_loss: 24.4218\n",
      "Epoch 341/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.3039 - val_loss: 24.0457\n",
      "Epoch 342/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.3023 - val_loss: 24.0957\n",
      "Epoch 343/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1852 - val_loss: 24.0844\n",
      "Epoch 344/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2506 - val_loss: 24.2440\n",
      "Epoch 345/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2230 - val_loss: 24.0459\n",
      "Epoch 346/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1343 - val_loss: 24.0293\n",
      "Epoch 347/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1784 - val_loss: 24.4238\n",
      "Epoch 348/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1187 - val_loss: 24.0803\n",
      "Epoch 349/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1751 - val_loss: 24.3957\n",
      "Epoch 350/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1927 - val_loss: 24.1531\n",
      "Epoch 351/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2704 - val_loss: 23.9847\n",
      "Epoch 352/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1435 - val_loss: 24.2118\n",
      "Epoch 353/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2884 - val_loss: 24.2444\n",
      "Epoch 354/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1529 - val_loss: 23.9948\n",
      "Epoch 355/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2369 - val_loss: 23.9854\n",
      "Epoch 356/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.2454 - val_loss: 23.9954\n",
      "Epoch 357/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.0986 - val_loss: 23.9280\n",
      "Epoch 358/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1936 - val_loss: 23.9267\n",
      "Epoch 359/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1398 - val_loss: 23.8800\n",
      "Epoch 360/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1457 - val_loss: 23.9912\n",
      "Epoch 361/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1793 - val_loss: 24.0282\n",
      "Epoch 362/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1250 - val_loss: 23.9933\n",
      "Epoch 363/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1615 - val_loss: 24.0870\n",
      "Epoch 364/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1009 - val_loss: 23.9729\n",
      "Epoch 365/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0996 - val_loss: 24.2498\n",
      "Epoch 366/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1796 - val_loss: 23.8868\n",
      "Epoch 367/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1490 - val_loss: 23.9583\n",
      "Epoch 368/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1881 - val_loss: 23.9142\n",
      "Epoch 369/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2227 - val_loss: 23.9743\n",
      "Epoch 370/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0815 - val_loss: 23.9819\n",
      "Epoch 371/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1233 - val_loss: 23.9722\n",
      "Epoch 372/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1296 - val_loss: 24.0561\n",
      "Epoch 373/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1460 - val_loss: 24.1536\n",
      "Epoch 374/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1948 - val_loss: 24.0243\n",
      "Epoch 375/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1490 - val_loss: 24.1344\n",
      "Epoch 376/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2847 - val_loss: 23.9720\n",
      "Epoch 377/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1093 - val_loss: 24.0735\n",
      "Epoch 378/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0870 - val_loss: 23.9659\n",
      "Epoch 379/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0879 - val_loss: 23.9641\n",
      "Epoch 380/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1190 - val_loss: 24.0052\n",
      "Epoch 381/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2347 - val_loss: 23.8841\n",
      "Epoch 382/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0730 - val_loss: 23.9393\n",
      "Epoch 383/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1805 - val_loss: 24.2830\n",
      "Epoch 384/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.2003 - val_loss: 23.9328\n",
      "Epoch 385/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0783 - val_loss: 24.0472\n",
      "Epoch 386/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0358 - val_loss: 24.0926\n",
      "Epoch 387/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0343 - val_loss: 24.2702\n",
      "Epoch 388/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.0759 - val_loss: 24.0706\n",
      "Epoch 389/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0178 - val_loss: 24.0390\n",
      "Epoch 390/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0293 - val_loss: 23.9277\n",
      "Epoch 391/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0940 - val_loss: 24.0105\n",
      "Epoch 392/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0233 - val_loss: 23.9369\n",
      "Epoch 393/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1068 - val_loss: 23.9455\n",
      "Epoch 394/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0432 - val_loss: 23.7618\n",
      "Epoch 395/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.7564 - val_loss: 24.3221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.1027 - val_loss: 23.9524\n",
      "Epoch 397/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.0640 - val_loss: 23.8583\n",
      "Epoch 398/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0687 - val_loss: 23.8508\n",
      "Epoch 399/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0263 - val_loss: 23.8202\n",
      "Epoch 400/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9840 - val_loss: 23.8715\n",
      "Epoch 401/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9910 - val_loss: 23.7915\n",
      "Epoch 402/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.0753 - val_loss: 23.7519\n",
      "Epoch 403/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9339 - val_loss: 23.9147\n",
      "Epoch 404/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9712 - val_loss: 23.7844\n",
      "Epoch 405/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.1171 - val_loss: 24.0412\n",
      "Epoch 406/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0579 - val_loss: 23.8642\n",
      "Epoch 407/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9353 - val_loss: 23.8411\n",
      "Epoch 408/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 18.9285 - val_loss: 23.7860\n",
      "Epoch 409/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.0826 - val_loss: 23.9236\n",
      "Epoch 410/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9047 - val_loss: 23.7958\n",
      "Epoch 411/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9485 - val_loss: 23.7847\n",
      "Epoch 412/1000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 19.0698 - val_loss: 24.1702\n",
      "Epoch 413/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0443 - val_loss: 23.8505\n",
      "Epoch 414/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9524 - val_loss: 23.7908\n",
      "Epoch 415/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 18.8800 - val_loss: 23.7452\n",
      "Epoch 416/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9604 - val_loss: 23.7549\n",
      "Epoch 417/1000\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.0515 - val_loss: 23.9492\n",
      "Epoch 418/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0447 - val_loss: 23.7751\n",
      "Epoch 419/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.9900 - val_loss: 23.7962\n",
      "Epoch 420/1000\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0393 - val_loss: 23.7922\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(32,activation='relu',input_shape=(x_train.shape[1],)))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "es = EarlyStopping(monitor='loss',mode='min',patience=25)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "model2.compile(optimizer=opt,loss='MAPE')\n",
    "history2 = model2.fit(x_train,y_train,epochs=1000,validation_data=(x_test,y_test),callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92603f",
   "metadata": {
    "id": "af92603f"
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dd17c",
   "metadata": {
    "id": "512dd17c"
   },
   "outputs": [],
   "source": [
    "n = x_train.shape[0]\n",
    "x_train = torch.tensor(x_train,dtype=torch.float)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float)\n",
    "x_test = torch.tensor(x_test,dtype=torch.float)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b05dcd",
   "metadata": {
    "id": "83b05dcd"
   },
   "outputs": [],
   "source": [
    "datasets = torch.utils.data.TensorDataset(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be3e9d",
   "metadata": {
    "id": "f8be3e9d"
   },
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(datasets,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28cd99",
   "metadata": {
    "id": "6a28cd99"
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b46cc",
   "metadata": {
    "id": "e89b46cc"
   },
   "outputs": [],
   "source": [
    "weight_n = x_train.shape[1]\n",
    "net = torch.nn.Sequential(torch.nn.Linear(weight_n,1))\n",
    "torch.nn.init.normal(net)\n",
    "torch.nn.init.constant_(net[0].bias, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddf854",
   "metadata": {
    "id": "e1ddf854"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_iter:\n",
    "        output = net(x)\n",
    "        l = loss(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch {} loss: {:.4f}\".format(epoch + 1, l.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6251b",
   "metadata": {
    "id": "86d6251b"
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "# y_train = tf.keras.utils.to_categorical(y_train,num_classes=10)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Conv2D(filters=32,kernel_size=(5,5),padding='Same',activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(256,activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(10,activation='softmax'))\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "\n",
    "model3.compile(optimizer=opt,loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b14f7",
   "metadata": {
    "id": "e98b14f7"
   },
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114df85",
   "metadata": {
    "id": "b114df85"
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(256, activation = \"relu\"))\n",
    "model4.add(Dense(64, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab4d3d",
   "metadata": {
    "id": "e6ab4d3d"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72456ea",
   "metadata": {
    "id": "b72456ea"
   },
   "outputs": [],
   "source": [
    "model4.compile(optimizer=opt,loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f0aa0",
   "metadata": {
    "id": "431f0aa0",
    "outputId": "d74b268e-9707-41d8-ddb4-53cc164f74b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 9s 4ms/step - loss: 2.2841 - accuracy: 0.5383 - val_loss: 1.1263 - val_accuracy: 0.8137\n"
     ]
    }
   ],
   "source": [
    "history4 = model4.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245950bd",
   "metadata": {
    "id": "245950bd"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf138f",
   "metadata": {
    "id": "afdf138f"
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e78e4a",
   "metadata": {
    "id": "92e78e4a",
    "outputId": "8665cae2-4379-4c03-e446-52c866811de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape,y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1abc81",
   "metadata": {
    "id": "ce1abc81"
   },
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "x_train = pad_sequences(x_train,maxlen=300)\n",
    "x_test = pad_sequences(x_test,maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6544ad9",
   "metadata": {
    "id": "a6544ad9"
   },
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(10000,32,input_length=(300)))\n",
    "model5.add(SimpleRNN(16,activation='relu'))\n",
    "model5.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363055b",
   "metadata": {
    "id": "b363055b",
    "outputId": "d54c1bf1-4319-4b96-d423-261e2d550297",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 300, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 16)                784       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,801\n",
      "Trainable params: 320,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a913060",
   "metadata": {
    "id": "3a913060",
    "outputId": "5e1c48fa-ca40-49a2-bcbb-8c1d0b50fc00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 300), (25000,), (25000, 300), (25000,))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape,y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592cd76",
   "metadata": {
    "id": "c592cd76"
   },
   "outputs": [],
   "source": [
    "model5.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139f062",
   "metadata": {
    "collapsed": true,
    "id": "1139f062",
    "outputId": "94f0e4cd-30c0-435e-95c5-0e9d0da627b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 63s 79ms/step - loss: 0.5746 - accuracy: 0.6969\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 62s 80ms/step - loss: 0.3633 - accuracy: 0.8478\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.2910 - accuracy: 0.8816\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.2431 - accuracy: 0.9042\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 66s 84ms/step - loss: 0.2202 - accuracy: 0.9171\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 65s 83ms/step - loss: 0.2014 - accuracy: 0.9266\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 62s 79ms/step - loss: 0.1791 - accuracy: 0.9326\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 63s 81ms/step - loss: 0.1707 - accuracy: 0.9374\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.1568 - accuracy: 0.9427\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 0.1436 - accuracy: 0.9487\n"
     ]
    }
   ],
   "source": [
    "history5 = model5.fit(x_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584fa5e",
   "metadata": {
    "id": "7584fa5e"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f881c",
   "metadata": {
    "id": "eb5f881c"
   },
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Embedding(10000,input_length=(300),output_dim=32))\n",
    "model6.add(LSTM(128))\n",
    "model6.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad5fd2",
   "metadata": {
    "id": "0cad5fd2"
   },
   "outputs": [],
   "source": [
    "model6.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf53d9f",
   "metadata": {
    "collapsed": true,
    "id": "3cf53d9f",
    "outputId": "934ddc47-94f1-4a85-889b-4d51a4e054f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 34/782 [>.............................] - ETA: 3:41 - loss: 0.6928 - accuracy: 0.5303"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[211], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history6 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel6\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history6 = model6.fit(x_train,y_train,epochs=3,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9ad17",
   "metadata": {
    "id": "fff9ad17"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5f21e",
   "metadata": {
    "id": "abc5f21e"
   },
   "outputs": [],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Embedding(10000,input_length=(300),output_dim=32))\n",
    "model7.add(GRU(128))\n",
    "model7.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75c95f",
   "metadata": {
    "id": "6c75c95f"
   },
   "outputs": [],
   "source": [
    "model7.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579590e",
   "metadata": {
    "id": "b579590e"
   },
   "outputs": [],
   "source": [
    "history7 = model7.fit(x_train,y_train,epochs=3,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc0985",
   "metadata": {
    "id": "11dc0985"
   },
   "source": [
    "### TEXT Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7191938e",
   "metadata": {
    "id": "7191938e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Buffalo Billâ€™s\n",
      "defunct\n",
      "who used to\n",
      "ride a watersmooth-silver\n",
      "stallion\n",
      "and break one two three four five pigeons just like that\n",
      "Jesus\n",
      "\n",
      "he was a handsome man\n",
      "and what i want to know is\n",
      "how do you like your blueeyed boy\n",
      "Mister Death\n",
      "\n",
      "Had I the heavenâ€™s embroidered cloths,\n",
      "Enwrought with golden and silver light,\n",
      "The blue and the dim and the dark cloths\n",
      "Of night and light and the half-light,\n",
      "I would spread the cloths under your feet:\n",
      "But I, being poor, have only my dreams;\n",
      "I have spread my dreams under your feet;\n",
      "Tread softly because you tread on my dreams.\n",
      "\n",
      "He clasps the crag with crooked hands;\n",
      "Close to the sun in lonely lands,\n",
      "Ringâ€™d with the azure world, he stands.\n",
      "\n",
      "The wrinkled sea beneath him crawls;\n",
      "He watches from his mountain walls,\n",
      "And like a thunderbolt he falls.\n",
      "\n",
      "Some say the world will end in fire,\n",
      "Some say in ice.\n",
      "From what Iâ€™ve tasted of desire\n",
      "I hold with those who favor fire.\n",
      "But if it had to perish twice,\n",
      "I think I know enough of hate\n",
      "To say that for destruction ice\n",
      "Is also great\n",
      "And would suffice.\n",
      "\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth;\n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim,\n",
      "Because it was grassy and wanted wear;\n",
      "Though as for that the passing there\n",
      "Had worn them really about the same,\n",
      "\n",
      "And both that morning equally lay\n",
      "In leaves no step had trodden black.\n",
      "Oh, I kept the first for another day!\n",
      "Yet knowing how way leads on to way,\n",
      "I doubted if I should ever come back.\n",
      "\n",
      "I shall be telling this with a sigh\n",
      "Somewhere ages and ages hence:\n",
      "Two roads diverged in a wood, and Iâ€”\n",
      "I took the one less traveled by,\n",
      "And that has made all the difference.\n",
      "hey There :  [[6], [2], [17], [1], [4], [6], [2], [11], [2]]\n",
      "hey There :  [[6, 2, 17, 1, 4, 6, 2, 11, 2]]\n",
      "1/1 [==============================] - 0s 416ms/step\n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "vanakam da maaplad\n",
      "mes as i\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Conv2D,MaxPooling2D,Input,Dense,Dropout,Flatten,Reshape,Embedding,GRU,LSTM,SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "with open ('C:/Users/sanja/Downloads/poems (1).txt') as f:\n",
    "    text = f.read()\n",
    "print(text)    \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(text)\n",
    "print(\"hey There : \",tokenizer.texts_to_sequences('hey There'))\n",
    "print(\"hey There : \",tokenizer.texts_to_sequences(['hey There']))\n",
    "\n",
    "data = np.array(tokenizer.texts_to_sequences([text]))[0] - 1\n",
    "data\n",
    "\n",
    "vo_len = len(tokenizer.word_index)\n",
    "seq_len = 100\n",
    "data = tf.data.Dataset.from_tensor_slices(data)\n",
    "data = data.batch(seq_len+1,drop_remainder=True)\n",
    "data = data.map(lambda x : (x[:-1],x[1:]))\n",
    "data = data.shuffle(1000).batch(64)\n",
    "\n",
    "\n",
    "tok = Sequential()\n",
    "tok.add(Embedding(vo_len,32))\n",
    "tok.add(GRU(100,return_sequences=True,dropout=0.1))\n",
    "tok.add(Dense(vo_len,activation='softmax'))\n",
    "\n",
    "tok.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "his=tok.fit(data,epochs=1000,verbose=False)\n",
    "\n",
    "def generator(s):\n",
    "    dat = np.array(tokenizer.texts_to_sequences([s]))-1\n",
    "    dat = tok.predict(dat)\n",
    "    out = np.argmax(dat,axis=-1)+1\n",
    "    out = tokenizer.sequences_to_texts(out)[0][-1]\n",
    "    return out\n",
    "\n",
    "def pred(s,loop=10):\n",
    "    for _ in range(loop):\n",
    "        s+=generator(s)\n",
    "    return s    \n",
    "\n",
    "print(pred('he was'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf18c40",
   "metadata": {},
   "source": [
    "CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55753ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 1.5388 - accuracy: 0.4399 - val_loss: 1.2515 - val_accuracy: 0.5522\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 1.1432 - accuracy: 0.5993 - val_loss: 1.0675 - val_accuracy: 0.6262\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.9943 - accuracy: 0.6534 - val_loss: 0.9741 - val_accuracy: 0.6584\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 0.8937 - accuracy: 0.6871 - val_loss: 0.9511 - val_accuracy: 0.6679\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 0.8165 - accuracy: 0.7145 - val_loss: 0.8903 - val_accuracy: 0.6899\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.7629 - accuracy: 0.7342 - val_loss: 0.8491 - val_accuracy: 0.7091\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.7125 - accuracy: 0.7505 - val_loss: 0.8647 - val_accuracy: 0.7068\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 0.6655 - accuracy: 0.7686 - val_loss: 0.8392 - val_accuracy: 0.7143\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 43s 28ms/step - loss: 0.6218 - accuracy: 0.7798 - val_loss: 0.8537 - val_accuracy: 0.7130\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.5889 - accuracy: 0.7931 - val_loss: 0.8524 - val_accuracy: 0.7248\n",
      "313/313 - 2s - loss: 0.8524 - accuracy: 0.7248 - 2s/epoch - 7ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,datasets,models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=datasets.cifar10.load_data()\n",
    "\n",
    "x_train,x_test=x_train/255.0,x_test/255.0\n",
    "\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64,activation=\"relu\"))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
    "\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "\n",
    "test_loss,test_acc=model.evaluate(x_test,y_test,verbose=2)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515ed19",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de81dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1000, 32)          320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322113 (1.23 MB)\n",
      "Trainable params: 322113 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 65s 204ms/step - loss: 0.5975 - accuracy: 0.6667 - val_loss: 0.4367 - val_accuracy: 0.8124\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 66s 212ms/step - loss: 0.3648 - accuracy: 0.8462 - val_loss: 0.3796 - val_accuracy: 0.8400\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 66s 209ms/step - loss: 0.2535 - accuracy: 0.8995 - val_loss: 0.4021 - val_accuracy: 0.8346\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 63s 203ms/step - loss: 0.1615 - accuracy: 0.9421 - val_loss: 0.4250 - val_accuracy: 0.8336\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 67s 213ms/step - loss: 0.0755 - accuracy: 0.9772 - val_loss: 0.5241 - val_accuracy: 0.8120\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.5490 - accuracy: 0.8036\n",
      "Test Loss: 0.5490, Test Accuracy: 0.8036\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=10000)\n",
    "\n",
    "from tensorflow.keras.layers import Dense,SimpleRNN,Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_words=1000\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32, input_length=max_words))  # Embedding layer\n",
    "model.add(SimpleRNN(units=32))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98873ac2",
   "metadata": {},
   "source": [
    "AUTO ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7d12c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 12s 13ms/step - loss: 0.0506 - val_loss: 0.0293\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0252 - val_loss: 0.0213\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0199 - val_loss: 0.0178\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0174 - val_loss: 0.0161\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0159 - val_loss: 0.0147\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0146 - val_loss: 0.0138\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 7ms/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0128 - val_loss: 0.0122\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0122 - val_loss: 0.0117\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0093 - val_loss: 0.0090\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0090 - val_loss: 0.0086\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Reshape the data for the autoencoder\n",
    "x_train = x_train.reshape(len(x_train), 784)\n",
    "x_test = x_test.reshape(len(x_test), 784)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_layer = Input(shape=(784,))\n",
    "encoder = Dense(128, activation='relu')(input_layer)\n",
    "encoder = Dense(64, activation='relu')(encoder)\n",
    "encoder = Dense(32, activation='relu')(encoder)\n",
    "\n",
    "decoder = Dense(64, activation='relu')(encoder)\n",
    "decoder = Dense(128, activation='relu')(decoder)\n",
    "decoder = Dense(784, activation='sigmoid')(decoder)\n",
    "\n",
    "autoencoder = Model(input_layer, decoder)\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(x_train, x_train, epochs=20, batch_size=128, validation_data=(x_test, x_test))\n",
    "\n",
    "# Plot the loss versus epoch curve\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4defe5",
   "metadata": {},
   "source": [
    "ANN VS CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "353887b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1779 - accuracy: 0.9466 - val_loss: 0.0720 - val_accuracy: 0.9771\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0614 - accuracy: 0.9811 - val_loss: 0.0517 - val_accuracy: 0.9824\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.0518 - val_accuracy: 0.9826\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0291 - accuracy: 0.9908 - val_loss: 0.0432 - val_accuracy: 0.9854\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.0440 - val_accuracy: 0.9854\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 52s 26ms/step - loss: 0.1738 - accuracy: 0.9491 - val_loss: 0.0808 - val_accuracy: 0.9748\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0606 - accuracy: 0.9823 - val_loss: 0.0511 - val_accuracy: 0.9840\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0403 - accuracy: 0.9880 - val_loss: 0.0494 - val_accuracy: 0.9838\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0285 - accuracy: 0.9912 - val_loss: 0.0400 - val_accuracy: 0.9865\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.0487 - val_accuracy: 0.9850\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3129 - accuracy: 0.9107 - val_loss: 0.1769 - val_accuracy: 0.9496\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1521 - accuracy: 0.9553 - val_loss: 0.1366 - val_accuracy: 0.9565\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1176 - accuracy: 0.9644 - val_loss: 0.1154 - val_accuracy: 0.9648\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0982 - accuracy: 0.9694 - val_loss: 0.1088 - val_accuracy: 0.9651\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0841 - accuracy: 0.9742 - val_loss: 0.1002 - val_accuracy: 0.9687\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>98.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN</td>\n",
       "      <td>96.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  accuracy\n",
       "0   CNN     98.40\n",
       "1   ANN     96.89"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense,Flatten\n",
    "\n",
    "## CNN\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data() \n",
    "\n",
    "x_train.shape\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "\n",
    "# Convert labels to integers\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to integers\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "## ANN\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data() \n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')/255.0\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "ann_model = Sequential()\n",
    "ann_model.add(Dense(32,activation='relu'))\n",
    "ann_model.add(Dense(64,activation='relu'))\n",
    "ann_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "ann_model.fit(x_train,y_train,epochs=5,validation_data=[x_test,y_test])\n",
    "\n",
    "## Comparing Results\n",
    "\n",
    "results = {\n",
    "    'model':['CNN','ANN'],\n",
    "    'accuracy':[98.4,96.89]\n",
    "}\n",
    "import pandas as pd\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
